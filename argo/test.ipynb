{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "xmlfile = pd.read_xml(\"/Volumes/Samsung_T5/argo/deltas.dfxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>execution_environment</th>\n",
       "      <th>image_filename</th>\n",
       "      <th>partition_offset</th>\n",
       "      <th>block_size</th>\n",
       "      <th>ftype</th>\n",
       "      <th>ftype_str</th>\n",
       "      <th>block_count</th>\n",
       "      <th>first_block</th>\n",
       "      <th>last_block</th>\n",
       "      <th>original_volume</th>\n",
       "      <th>fileobject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Disk image difference set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Volumes/Storage/mtlaam/tmp/workflow/diskprint...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>105906176.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ntfs</td>\n",
       "      <td>5216767.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5216766.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1048576.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ntfs</td>\n",
       "      <td>25599.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25598.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        type  execution_environment  \\\n",
       "0  Disk image difference set                    NaN   \n",
       "1                       None                    NaN   \n",
       "2                       None                    NaN   \n",
       "3                       None                    NaN   \n",
       "4                       None                    NaN   \n",
       "\n",
       "                                      image_filename  partition_offset  \\\n",
       "0                                               None               NaN   \n",
       "1                                               None               NaN   \n",
       "2  /Volumes/Storage/mtlaam/tmp/workflow/diskprint...               NaN   \n",
       "3                                               None       105906176.0   \n",
       "4                                               None         1048576.0   \n",
       "\n",
       "   block_size  ftype ftype_str  block_count  first_block  last_block  \\\n",
       "0         NaN    NaN      None          NaN          NaN         NaN   \n",
       "1         NaN    NaN      None          NaN          NaN         NaN   \n",
       "2         NaN    NaN      None          NaN          NaN         NaN   \n",
       "3      4096.0    1.0      ntfs    5216767.0          0.0   5216766.0   \n",
       "4      4096.0    1.0      ntfs      25599.0          0.0     25598.0   \n",
       "\n",
       "   original_volume  fileobject  \n",
       "0              NaN         NaN  \n",
       "1              NaN         NaN  \n",
       "2              NaN         NaN  \n",
       "3              NaN         NaN  \n",
       "4              NaN         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmlfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfxml\n",
    "iters = dfxml.iter_dfxml('/Volumes/Samsung_T5/M57/pat-2009-12-10.raw.xml', preserve_elements=True)\n",
    "obj_list = []\n",
    "for obj in iters:\n",
    "    obj_list.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49745\n"
     ]
    }
   ],
   "source": [
    "print(len(obj_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = obj_list[34485] #28982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WINDOWS/system32/dllcache/fpexedll.dll'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.filename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TIMETAGLIST', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_byte_runs', '_tags', 'allocated', 'allocated_inode', 'allocated_name', 'atime', 'byte_runs', 'compressed', 'content_for_run', 'contents', 'crtime', 'ctime', 'dtime', 'encrypted', 'ext', 'file_present', 'filename', 'filesize', 'frag_start_sector', 'fragments', 'gid', 'has_contents', 'has_sector', 'has_tag', 'hashdigest', 'imagefile', 'inode', 'is_dir', 'is_file', 'is_virtual', 'libmagic', 'md5', 'meta_type', 'mode', 'mtime', 'name_type', 'partition', 'savefile', 'sha1', 'sha256', 'tag', 'tempfile', 'times', 'uid', 'volume', 'xml_element']\n"
     ]
    }
   ],
   "source": [
    "print(dir(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<dfxml.byte_run at 0x7fd4023475b0>,\n",
       " <dfxml.byte_run at 0x7fd4023475e0>,\n",
       " <dfxml.byte_run at 0x7fd402347610>,\n",
       " <dfxml.byte_run at 0x7fd402347640>,\n",
       " <dfxml.byte_run at 0x7fd402347670>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.byte_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'decode_sax_attributes', 'decode_xml_attributes', 'extra_len', 'file_offset', 'fs_offset', 'has_sector', 'hashdigest', 'img_offset', 'len', 'sector_count', 'sector_size', 'start_sector', 'uncompressed_len']\n"
     ]
    }
   ],
   "source": [
    "print(dir(a.byte_runs()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.byte_runs()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = vars(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'img_offset': 5673254400, 'file_offset': 0, 'len': None, 'sector_size': 512, 'hashdigest': {}, 'fs_offset': 5673222144, 'uncompressed_len': 8192}\n"
     ]
    }
   ],
   "source": [
    "print(type(c))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7fd4a7347b30>\n"
     ]
    }
   ],
   "source": [
    "iterlist = (row for row in iters)\n",
    "print(iterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist= [obj for obj in iters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for what in iterlist:\n",
    "    print(what)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPIpool.py\n",
    "\n",
    "from mpi4py.futures import MPIPoolExecutor\n",
    "import math\n",
    "import textwrap\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import repeat\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pprint\n",
    "from pprint import pprint\n",
    "import dfxml \n",
    "\n",
    "def compute_hash(range_tuple):\n",
    "    img_path = '/home/oadegbeh/M57/pat-2009-12-10.raw.raw'\n",
    "    sector_hash_list = []\n",
    "    print(range_tuple[0], range_tuple[1])\n",
    "    sector_size = 512\n",
    "    img_h = open(img_path, 'rb')\n",
    "    for pointer in range(range_tuple[0], range_tuple[1]):\n",
    "                #print(pointer)\n",
    "                img_offset = pointer*sector_size\n",
    "                img_h.seek(img_offset)\n",
    "                fsector = img_h.read(sector_size)\n",
    "                sector_md5 = hashlib.md5(fsector).hexdigest()\n",
    "                sector_hash_list.append((img_offset,sector_md5))\n",
    "    return sector_hash_list\n",
    "\n",
    "def process_object_sectors(range_tuple):\n",
    "    sector_size = 512\n",
    "    img_csv = \"/Volumes/Samsung_T5/argo/pat2.csv\"\n",
    "    img_csv_df = pd.read_csv(img_csv)\n",
    "    global options\n",
    "    files_cols = ['obj_id', 'partition','inode','filename','filesize']\n",
    "    block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "    file_df = pd.DataFrame(columns=files_cols)\n",
    "    ebyterun_df = pd.DataFrame(columns=block_hashes_cols)\n",
    "    return_list = []\n",
    "    for obj in range(range_tuple[0], range_tuple[1]):\n",
    "        # Filter out specific filenames create by TSK that are not of use\n",
    "        print(\"processing this  %s\" % str(obj))\n",
    "        print(type(obj))\n",
    "        #if int(obj._tags['id']) == id:# and obj.filesize() == False:\n",
    "        #counter += 1\n",
    "        #print(obj.filename())\n",
    "        data = [str(obj), obj.partition(),obj.inode(), obj.filename(),  obj.filesize()] #obj._tags['id']\n",
    "        file_df.loc[len(file_df.index)] = data\n",
    "        print(data)\n",
    "        byterun = []\n",
    "        persist_img_offset = 0\n",
    "        persist_fs_offset = 0\n",
    "        persist_file_offset = 0\n",
    "        remaining_len = obj.filesize()\n",
    "        #ebyterun_df = pd.DataFrame(columns=['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1'])\n",
    "        for ebyterun in obj.byte_runs():\n",
    "            #pprint(vars(ebyterun)) \n",
    "            #byterun.append([ebyterun.img_offset,ebyterun.file_offset, ebyterun.len, ebyterun.fs_offset])\n",
    "            byterun.append(ebyterun.file_offset)\n",
    "            persist_file_offset = ebyterun.file_offset\n",
    "            byterun.append(ebyterun.len)\n",
    "            if hasattr(ebyterun,'img_offset'): \n",
    "                byterun.append(ebyterun.img_offset)\n",
    "                if ebyterun.img_offset != None: persist_img_offset = ebyterun.img_offset\n",
    "            if hasattr(ebyterun,'fs_offset'): \n",
    "                byterun.append(ebyterun.fs_offset)\n",
    "                if ebyterun.fs_offset != None: persist_fs_offset = ebyterun.fs_offset\n",
    "            if hasattr(ebyterun,'len'): \n",
    "                byterun.append(ebyterun.len)\n",
    "                if ebyterun.len != None: persist_len = ebyterun.len\n",
    "            #print(\"img_offset\", img_offset, \"file offset\", ebyterun.file_offset)\n",
    "            #print(\"file_offset\", \"len\", \"img_offset\", \"fs_offset\")\n",
    "            byterun_start = int(persist_img_offset / sector_size)\n",
    "            byterun_end = int((math.ceil((persist_img_offset + ebyterun.len) / sector_size))) - 1\n",
    "            len_run = np.arange(persist_len,0,-sector_size) #remaining_len\n",
    "            sector_run = np.full(len(len_run), sector_size)\n",
    "            len_run = np.minimum(len_run,sector_run)\n",
    "            #len_run = min(sector_size,len_run.all())\n",
    "            #print(\"len_run\",len(len_run), len(len_run)*sector_size)\n",
    "            #print(\"byterun_start\", byterun_start,persist_img_offset, \"byterun_end\", byterun_end,  \"len\", ebyterun.len)\n",
    "            #print(\"ebyterun_df\", ebyterun_df.shape, \"img_csv\", img_csv_df.loc[byterun_start:byterun_end,'img_offset'].shape )\n",
    "            #print(img_csv_df.loc[byterun_start:byterun_end,:])\n",
    "            #print(\"img_csv_len\",len(img_csv_df.loc[byterun_start:byterun_end, :]))\n",
    "            ebyterun_df.loc[:,'img_sector_offset'] = np.arange(len(img_csv_df))\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'obj_id'] = str(obj) #obj._tags['id']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'img_offset'] = img_csv_df.loc[byterun_start:byterun_end,'img_offset']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'fs_offset'] = np.arange(persist_fs_offset,persist_fs_offset+ebyterun.len,sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'file_offset'] = np.arange(persist_file_offset,persist_file_offset+ebyterun.len, sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'len'] = len_run\n",
    "            #print(\"remaining_len\",remaining_len)\n",
    "            #remaining_len-=sector_size\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'md5'] = img_csv_df.loc[byterun_start:byterun_end,'md5']\n",
    "            return_list.append((file_df,ebyterun_df))\n",
    "        print(\"done processing %s\" % str(obj))\n",
    "    return return_list   \n",
    "\n",
    "def determine_subranges(fullrange, num_subranges):\n",
    "    \"\"\"\n",
    "    Break fullrange up into smaller sets of ranges that cover all\n",
    "    the same numbers.\n",
    "    \"\"\"\n",
    "    subranges = []\n",
    "    inc = fullrange[1] // num_subranges\n",
    "    for i in range(fullrange[0], fullrange[1], inc):\n",
    "        subranges.append( (i, min(i+inc, fullrange[1])) )\n",
    "    return( subranges )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #sector_size = 512\n",
    "    img_path = '/Volumes/Samsung_T5/M57/pat-2009-12-10.raw.xml'\n",
    "    object_number = dfxml.iter_dfxml(img_path, preserve_elements=True)\n",
    "    img_objcount = len(list(object_number))\n",
    "    fullrange = (0, img_objcount)\n",
    "    num_subranges = 1000\n",
    "    subranges = determine_subranges(fullrange, num_subranges)\n",
    "\n",
    "    executor = MPIPoolExecutor()\n",
    "    sectors_df_list = executor.map(process_object_sectors, subranges)\n",
    "    #files_df_list = executor.map(process_object_files, subranges)\n",
    "    executor.shutdown()\n",
    "    grand_sectors_df = pd.DataFrame()\n",
    "    grand_files_df = pd.DataFrame()\n",
    "    # flatten the list of lists\n",
    "    for df in sectors_df_list:\n",
    "        grand_files_df = pd.concat([grand_files_df, df[0]])\n",
    "        grand_sectors_df = pd.concat([grand_sectors_df, df[1]])\n",
    "    #for df in files_df_list:\n",
    "    #    grand_files_df = pd.concat([grand_files_df, df])\n",
    "    #print(textwrap.fill(str(primes),80))\n",
    "    con = sqlite3.connect(\"/Volumes/Samsung_T5/M57/pat.db\")\n",
    "    grand_sectors_df.to_sql('block_hashes', con, index=True)\n",
    "    grand_files_df.to_sql('files', con, index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LATEST VERSION\n",
    "# MPIpool.py\n",
    "\n",
    "from mpi4py.futures import MPIPoolExecutor\n",
    "import math\n",
    "import textwrap\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import repeat\n",
    "#import sqlite3\n",
    "import numpy as np\n",
    "import pprint\n",
    "from pprint import pprint\n",
    "import dfxml \n",
    "\n",
    "def compute_hash(range_tuple):\n",
    "    img_path = '/home/oadegbeh/M57/pat-2009-12-10.raw.raw'\n",
    "    sector_hash_list = []\n",
    "    print(range_tuple[0], range_tuple[1])\n",
    "    sector_size = 512\n",
    "    img_h = open(img_path, 'rb')\n",
    "    for pointer in range(range_tuple[0], range_tuple[1]):\n",
    "                #print(pointer)\n",
    "                img_offset = pointer*sector_size\n",
    "                img_h.seek(img_offset)\n",
    "                fsector = img_h.read(sector_size)\n",
    "                sector_md5 = hashlib.md5(fsector).hexdigest()\n",
    "                sector_hash_list.append((img_offset,sector_md5))\n",
    "    return sector_hash_list\n",
    "\n",
    "def process_object_sectors(objlist):\n",
    "    sector_size = 512\n",
    "    img_csv = \"/home/oadegbeh/M57/pat2.csv\"\n",
    "    img_csv_df = pd.read_csv(img_csv)\n",
    "    global options\n",
    "    files_cols = ['obj_id', 'partition','inode','filename','filesize']\n",
    "    block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "    file_df = pd.DataFrame(columns=files_cols)\n",
    "    ebyterun_df = pd.DataFrame(columns=block_hashes_cols)\n",
    "    return_list = []\n",
    "    for obj in objlist:#range(range_tuple[0], range_tuple[1]):\n",
    "        # Filter out specific filenames create by TSK that are not of use\n",
    "        print(\"processing %s\" % obj._tags['id']) #str(obj)\n",
    "        #print(type(obj))\n",
    "        #if int(obj._tags['id']) == id:# and obj.filesize() == False:\n",
    "        #counter += 1\n",
    "        #print(obj.filename())\n",
    "        data = [obj._tags['id'], obj.partition(),obj.inode(), obj.filename(),  obj.filesize()] #\n",
    "        file_df.loc[len(file_df.index)] = data\n",
    "        print(data)\n",
    "        byterun = []\n",
    "        persist_img_offset = 0\n",
    "        persist_fs_offset = 0\n",
    "        persist_file_offset = 0\n",
    "        remaining_len = obj.filesize()\n",
    "        #ebyterun_df = pd.DataFrame(columns=['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1'])\n",
    "        for ebyterun in obj.byte_runs():\n",
    "            #pprint(vars(ebyterun)) \n",
    "            #byterun.append([ebyterun.img_offset,ebyterun.file_offset, ebyterun.len, ebyterun.fs_offset])\n",
    "            byterun.append(ebyterun.file_offset)\n",
    "            persist_file_offset = ebyterun.file_offset\n",
    "            byterun.append(ebyterun.len)\n",
    "            if hasattr(ebyterun,'img_offset'): \n",
    "                byterun.append(ebyterun.img_offset)\n",
    "                if ebyterun.img_offset != None: persist_img_offset = ebyterun.img_offset\n",
    "            if hasattr(ebyterun,'fs_offset'): \n",
    "                byterun.append(ebyterun.fs_offset)\n",
    "                if ebyterun.fs_offset != None: persist_fs_offset = ebyterun.fs_offset\n",
    "            if hasattr(ebyterun,'len'): \n",
    "                byterun.append(ebyterun.len)\n",
    "                if ebyterun.len != None: persislen = ebyterun.len\n",
    "            #print(\"img_offset\", img_offset, \"file offset\", ebyterun.file_offset)\n",
    "            #print(\"file_offset\", \"len\", \"img_offset\", \"fs_offset\")\n",
    "            byterun_start = int(persist_img_offset / sector_size)\n",
    "            byterun_end = int((math.ceil((persist_img_offset + ebyterun.len) / sector_size))) - 1\n",
    "            len_run = np.arange(persist_len,0,-sector_size) #remaining_len\n",
    "            sector_run = np.full(len(len_run), sector_size)\n",
    "            len_run = np.minimum(len_run,sector_run)\n",
    "            #len_run = min(sector_size,len_run.all())\n",
    "            #print(\"len_run\",len(len_run), len(len_run)*sector_size)\n",
    "            #print(\"byterun_start\", byterun_start,persist_img_offset, \"byterun_end\", byterun_end,  \"len\", ebyterun.len)\n",
    "            #print(\"ebyterun_df\", ebyterun_df.shape, \"img_csv\", img_csv_df.loc[byterun_start:byterun_end,'img_offset'].shape )\n",
    "            #print(img_csv_df.loc[byterun_start:byterun_end,:])\n",
    "            #print(\"img_csv_len\",len(img_csv_df.loc[byterun_start:byterun_end, :]))\n",
    "            ebyterun_df.loc[:,'img_sector_offset'] = np.arange(len(img_csv_df))\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'obj_id'] = str(obj) #obj._tags['id']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'img_offset'] = img_csv_df.loc[byterun_start:byterun_end,'img_offset']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'fs_offset'] = np.arange(persist_fs_offset,persist_fs_offset+ebyterun.len,sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'file_offset'] = np.arange(persist_file_offset,persist_file_offset+ebyterun.len, sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'len'] = len_run\n",
    "            #print(\"remaining_len\",remaining_len)\n",
    "            #remaining_len-=sector_size\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'md5'] = img_csv_df.loc[byterun_start:byterun_end,'md5']\n",
    "            return_list.append((file_df,ebyterun_df))\n",
    "        print(\"done processing %s\" % obj._tags['id']) #str(obj)\n",
    "    return return_list   \n",
    "\n",
    "def determine_subranges(obj_list, num_subranges):#fullrange\n",
    "    \"\"\"\n",
    "    Break fullrange up into smaller sets of ranges that cover all\n",
    "    the same numbers.\n",
    "    \"\"\"\n",
    "    subranges = []\n",
    "    inc =  len(obj_list)// num_subranges #fullrange[1]\n",
    "    for i in range(0, len(obj_list), inc):#fullrange[0], fullrange[1]\n",
    "        subranges.append( (obj_list[i], obj_list[min(i+inc,len(obj_list)-1 )]) )#fullrange[1]\n",
    "    return( subranges )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #sector_size = 512\n",
    "    img_path = '/home/oadegbeh/M57/pat-2009-12-10.raw.xml'\n",
    "    object_gen = dfxml.iter_dfxml(img_path, preserve_elements=True)\n",
    "    obj_list = []\n",
    "    for each_obj in object_gen:\n",
    "        obj_list.append(each_obj)\n",
    "    #img_objcount = len(obj_list)\n",
    "    #fullrange = (0, img_objcount)\n",
    "    num_subranges = 1000\n",
    "    subranges = determine_subranges(obj_list, num_subranges)#fullrange\n",
    "\n",
    "    executor = MPIPoolExecutor()\n",
    "    sectors_df_list = executor.map(process_object_sectors, subranges)\n",
    "    #files_df_list = executor.map(process_object_files, subranges)\n",
    "    executor.shutdown()\n",
    "    grand_sectors_df = pd.DataFrame()\n",
    "    grand_files_df = pd.DataFrame()\n",
    "    # flatten the list of lists\n",
    "    for df in sectors_df_list: #COULD THIS BE A PROBLEM POINT?\n",
    "        grand_files_df = pd.concat([grand_files_df, df[0]])\n",
    "        grand_sectors_df = pd.concat([grand_sectors_df, df[1]])\n",
    "    #for df in files_df_list:\n",
    "    #    grand_files_df = pd.concat([grand_files_df, df])\n",
    "    #print(textwrap.fill(str(primes),80))\n",
    "    con = sqlite3.connect(\"/scratch/oadegbeh/pat.db\")\n",
    "    grand_sectors_df.to_sql('block_hashes', con, index=True)\n",
    "    grand_files_df.to_sql('files', con, index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('scan_match_validate_all_mpi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9897bb9d3ea6b47cb59a3fa29a33cee83df786d8baa248772bb7eb2ce311867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
