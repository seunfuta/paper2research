{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "xmlfile = pd.read_xml(\"/Volumes/Samsung_T5/argo/deltas.dfxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>execution_environment</th>\n",
       "      <th>image_filename</th>\n",
       "      <th>partition_offset</th>\n",
       "      <th>block_size</th>\n",
       "      <th>ftype</th>\n",
       "      <th>ftype_str</th>\n",
       "      <th>block_count</th>\n",
       "      <th>first_block</th>\n",
       "      <th>last_block</th>\n",
       "      <th>original_volume</th>\n",
       "      <th>fileobject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Disk image difference set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Volumes/Storage/mtlaam/tmp/workflow/diskprint...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>105906176.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ntfs</td>\n",
       "      <td>5216767.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5216766.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1048576.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ntfs</td>\n",
       "      <td>25599.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25598.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        type  execution_environment  \\\n",
       "0  Disk image difference set                    NaN   \n",
       "1                       None                    NaN   \n",
       "2                       None                    NaN   \n",
       "3                       None                    NaN   \n",
       "4                       None                    NaN   \n",
       "\n",
       "                                      image_filename  partition_offset  \\\n",
       "0                                               None               NaN   \n",
       "1                                               None               NaN   \n",
       "2  /Volumes/Storage/mtlaam/tmp/workflow/diskprint...               NaN   \n",
       "3                                               None       105906176.0   \n",
       "4                                               None         1048576.0   \n",
       "\n",
       "   block_size  ftype ftype_str  block_count  first_block  last_block  \\\n",
       "0         NaN    NaN      None          NaN          NaN         NaN   \n",
       "1         NaN    NaN      None          NaN          NaN         NaN   \n",
       "2         NaN    NaN      None          NaN          NaN         NaN   \n",
       "3      4096.0    1.0      ntfs    5216767.0          0.0   5216766.0   \n",
       "4      4096.0    1.0      ntfs      25599.0          0.0     25598.0   \n",
       "\n",
       "   original_volume  fileobject  \n",
       "0              NaN         NaN  \n",
       "1              NaN         NaN  \n",
       "2              NaN         NaN  \n",
       "3              NaN         NaN  \n",
       "4              NaN         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmlfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfxml\n",
    "iters = dfxml.iter_dfxml('/Volumes/Samsung_T5/M57/pat-2009-12-10.raw.xml', preserve_elements=True)\n",
    "obj_list = []\n",
    "for obj in iters:\n",
    "    obj_list.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49745\n"
     ]
    }
   ],
   "source": [
    "print(len(obj_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = obj_list[49] #28982, 34485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Documents and Settings/Administrator/Application Data/Microsoft/CryptnetUrlCache/MetaData/C554DCF706A5AAB8B360FAD227EAB9C7'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.filename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imagefile': None, 'hashdigest': {'md5': '27ddb3c1d0fe517f61ba8d96af2c09e4', 'sha1': 'dc6398cc1b0a3e0d5a52fbf25b92b81375556a25'}, '_tags': {'inode': '17658', 'parent_object': None, 'filename': 'Documents and Settings/Administrator/Application Data/Microsoft/CryptnetUrlCache/MetaData/C554DCF706A5AAB8B360FAD227EAB9C7', 'partition': '1', 'id': '50', 'name_type': 'r', 'filesize': '100', 'alloc': '1', 'used': '1', 'meta_type': '1', 'mode': '511', 'nlink': '2', 'uid': '0', 'gid': '0', 'mtime': '2009-11-10T00:56:36Z', 'ctime': '2009-11-10T00:56:36Z', 'atime': '2009-11-10T00:56:36Z', 'crtime': '2009-11-10T00:56:36Z', 'seq': '1', 'byte_run': '', 'byte_runs': None, 'md5': '27ddb3c1d0fe517f61ba8d96af2c09e4', 'sha1': 'dc6398cc1b0a3e0d5a52fbf25b92b81375556a25'}, '_byte_runs': [<dfxml.byte_run object at 0x7fc2812f4100>], 'volume': None, 'xml_element': <Element '{http://www.forensicswiki.org/wiki/Category:Digital_Forensics_XML}fileobject' at 0x7fc279990d60>}\n"
     ]
    }
   ],
   "source": [
    "print(vars(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TIMETAGLIST', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_byte_runs', '_tags', 'allocated', 'allocated_inode', 'allocated_name', 'atime', 'byte_runs', 'compressed', 'content_for_run', 'contents', 'crtime', 'ctime', 'dtime', 'encrypted', 'ext', 'file_present', 'filename', 'filesize', 'frag_start_sector', 'fragments', 'gid', 'has_contents', 'has_sector', 'has_tag', 'hashdigest', 'imagefile', 'inode', 'is_dir', 'is_file', 'is_virtual', 'libmagic', 'md5', 'meta_type', 'mode', 'mtime', 'name_type', 'partition', 'savefile', 'sha1', 'sha256', 'tag', 'tempfile', 'times', 'uid', 'volume', 'xml_element']\n"
     ]
    }
   ],
   "source": [
    "print(dir(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<dfxml.byte_run at 0x7fc2812f4100>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.byte_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'decode_sax_attributes', 'decode_xml_attributes', 'extra_len', 'file_offset', 'fs_offset', 'has_sector', 'hashdigest', 'img_offset', 'len', 'sector_count', 'sector_size', 'start_sector', 'uncompressed_len']\n"
     ]
    }
   ],
   "source": [
    "print(dir(a.byte_runs()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.byte_runs()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = vars(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'img_offset': 32256, 'file_offset': 0, 'len': 100, 'sector_size': 512, 'hashdigest': {}, 'fs_offset': 0, 'type': 'resident'}\n"
     ]
    }
   ],
   "source": [
    "print(type(c))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imagefile': None, 'hashdigest': {'md5': 'b4914a686146393c690dc0f78a0d5328', 'sha1': 'd317816485216cce9fcc44cd3cc5f46508ceab5a'}, '_tags': {'inode': '7685', 'parent_object': None, 'filename': 'WINDOWS/system32/dllcache/fpexedll.dll', 'partition': '1', 'id': '34486', 'name_type': 'r', 'filesize': '20541', 'alloc': '1', 'used': '1', 'compressed': '1', 'meta_type': '1', 'mode': '511', 'nlink': '1', 'uid': '0', 'gid': '0', 'mtime': '2003-03-25T00:52:04Z', 'ctime': '2009-11-09T00:31:15Z', 'atime': '2009-12-08T00:05:42Z', 'crtime': '2009-11-09T00:31:15Z', 'seq': '2', 'byte_run': '', 'byte_runs': None, 'md5': 'b4914a686146393c690dc0f78a0d5328', 'sha1': 'd317816485216cce9fcc44cd3cc5f46508ceab5a'}, '_byte_runs': [<dfxml.byte_run object at 0x7fc25ae60220>, <dfxml.byte_run object at 0x7fc25ae60250>, <dfxml.byte_run object at 0x7fc25ae60280>, <dfxml.byte_run object at 0x7fc25ae602b0>, <dfxml.byte_run object at 0x7fc25ae602e0>], 'volume': None, 'xml_element': <Element '{http://www.forensicswiki.org/wiki/Category:Digital_Forensics_XML}fileobject' at 0x7fc25ae3ce00>}\n",
      "34486\n"
     ]
    }
   ],
   "source": [
    "print(vars(a))\n",
    "print(a._tags['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_offset': 32256, 'file_offset': 0, 'len': 100, 'sector_size': 512, 'hashdigest': {}, 'fs_offset': 0, 'type': 'resident'}\n",
      "start 63 stop 63\n",
      "id 50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "sector_size = 512\n",
    "block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "df = pd.DataFrame(columns=block_hashes_cols)\n",
    "p_img_offset = 0\n",
    "p_fs_offset = 0\n",
    "p_file_offset = 0\n",
    "p_len = 0\n",
    "print(c)\n",
    "p_img_offset = c['img_offset'] if c['img_offset'] else p_img_offset\n",
    "p_fs_offset = c['fs_offset'] if c['fs_offset'] else p_fs_offset\n",
    "p_file_offset = c['file_offset'] if c['file_offset'] else p_file_offset\n",
    "p_len = c['len'] if c['len'] else c['uncompressed_len'] if c['uncompressed_len'] else p_len\n",
    "start = p_img_offset//sector_size\n",
    "stop = (p_img_offset+p_len)//sector_size\n",
    "print(\"start\", start, \"stop\", stop, )\n",
    "df.loc[:,'img_sector_offset'] = np.arange(start, stop)\n",
    "print(\"id\", a._tags['id'])\n",
    "df.loc[:,'obj_id'] = a._tags['id'] #obj._tags['id']\n",
    "df.loc[:,'img_offset'] = np.arange(p_img_offset,p_img_offset+p_len,sector_size)\n",
    "df.loc[:,'fs_offset'] = np.arange(p_fs_offset,p_fs_offset+p_len,sector_size)\n",
    "df.loc[:,'file_offset'] = np.arange(p_file_offset,p_file_offset+p_len, sector_size)\n",
    "len_run = np.arange(p_len,0,-sector_size) #remaining_len\n",
    "sector_run = np.full(len(len_run), sector_size)\n",
    "len_run = np.minimum(len_run,sector_run)\n",
    "df.loc[:,'len'] = len_run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_id</th>\n",
       "      <th>img_offset</th>\n",
       "      <th>fs_offset</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>len</th>\n",
       "      <th>md5</th>\n",
       "      <th>sha1</th>\n",
       "      <th>img_sector_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [obj_id, img_offset, fs_offset, file_offset, len, md5, sha1, img_sector_offset]\n",
       "Index: []"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_offset': 32256, 'file_offset': 0, 'len': 100, 'sector_size': 512, 'hashdigest': {}, 'fs_offset': 0, 'type': 'resident'}\n",
      "image offset 32256\n",
      "fs_offset 0\n",
      "file_offset 0\n",
      "len 100\n",
      "start 63 stop 64\n",
      "id 50\n",
      "img_offsetE 32356 fs_offset 100 file_offset 100\n"
     ]
    }
   ],
   "source": [
    "import dfxml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "iters = dfxml.iter_dfxml('/Volumes/Samsung_T5/M57/pat-2009-12-10.raw.xml', preserve_elements=True)\n",
    "obj_list = []\n",
    "for obj in iters:\n",
    "    obj_list.append(obj)\n",
    "sector_size = 512   \n",
    "fobj = obj_list[49]\n",
    "block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "block_hash_df = pd.DataFrame(columns=block_hashes_cols)\n",
    "p_img_offset = 0\n",
    "p_fs_offset = 0\n",
    "p_file_offset = 0\n",
    "p_len = 0#\n",
    "for byterun in fobj.byte_runs():\n",
    "    c = vars(byterun)\n",
    "    print(c)\n",
    "    p_img_offset = c['img_offset'] if (hasattr(byterun,'img_offset') and byterun.img_offset != None) else p_img_offset\n",
    "    print(\"image offset\", p_img_offset)\n",
    "    p_fs_offset = c['fs_offset'] if (hasattr(byterun,'fs_offset') and byterun.fs_offset != None) else p_fs_offset\n",
    "    print(\"fs_offset\", p_fs_offset)\n",
    "    p_file_offset = c['file_offset'] if (hasattr(byterun,'file_offset') and byterun.file_offset != None) else p_file_offset\n",
    "    print(\"file_offset\",p_file_offset)\n",
    "    p_len = byterun.len if (hasattr(byterun,'len') and byterun.len != None) else byterun.uncompressed_len if (hasattr(byterun,'uncompressed_len')and byterun.uncompressed_len != None) else p_len\n",
    "    print(\"len\", p_len)\n",
    "    start = p_img_offset//sector_size\n",
    "    stop = (p_img_offset+max(p_len,sector_size))//sector_size #(p_img_offset+max(p_len, sector_size))//sector_size\n",
    "    print(\"start\", start, \"stop\", stop, )\n",
    "    cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    df.loc[:,'img_sector_offset'] = np.arange(start, stop)\n",
    "    print(\"id\", a._tags['id'])\n",
    "    df.loc[:,'obj_id'] = a._tags['id'] #obj._tags['id']\n",
    "    df.loc[:,'img_offset'] = np.arange(p_img_offset,p_img_offset+p_len,sector_size)\n",
    "    df.loc[:,'fs_offset'] = np.arange(p_fs_offset,p_fs_offset+p_len,sector_size)\n",
    "    df.loc[:,'file_offset'] = np.arange(p_file_offset,p_file_offset+p_len, sector_size)\n",
    "    len_run = np.arange(p_len,0,-sector_size) #remaining_len\n",
    "    sector_run = np.full(len(len_run), sector_size)\n",
    "    len_run = np.minimum(len_run,sector_run)\n",
    "    df.loc[:,'len'] = len_run\n",
    "    block_hash_df = pd.concat([block_hash_df, df])\n",
    "    p_img_offset += p_len\n",
    "    p_fs_offset += p_len\n",
    "    p_file_offset += p_len\n",
    "    print(\"img_offsetE\", p_img_offset, \"fs_offset\", p_fs_offset, \"file_offset\", p_file_offset)\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_id</th>\n",
       "      <th>img_offset</th>\n",
       "      <th>fs_offset</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>len</th>\n",
       "      <th>md5</th>\n",
       "      <th>sha1</th>\n",
       "      <th>img_sector_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>32256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  obj_id img_offset fs_offset file_offset  len  md5 sha1  img_sector_offset\n",
       "0     50      32256         0           0  100  NaN  NaN               63.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_hash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "#fobj.filesize()\n",
    "print(len(block_hash_df))\n",
    "#fobj.filename()\n",
    "#fobj.byte_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7fd4a7347b30>\n"
     ]
    }
   ],
   "source": [
    "iterlist = (row for row in iters)\n",
    "print(iterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist= [obj for obj in iters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for what in iterlist:\n",
    "    print(what)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPIpool.py\n",
    "\n",
    "from mpi4py.futures import MPIPoolExecutor\n",
    "import math\n",
    "import textwrap\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import repeat\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pprint\n",
    "from pprint import pprint\n",
    "import dfxml \n",
    "\n",
    "def compute_hash(range_tuple):\n",
    "    img_path = '/home/oadegbeh/M57/pat-2009-12-10.raw.raw'\n",
    "    sector_hash_list = []\n",
    "    print(range_tuple[0], range_tuple[1])\n",
    "    sector_size = 512\n",
    "    img_h = open(img_path, 'rb')\n",
    "    for pointer in range(range_tuple[0], range_tuple[1]):\n",
    "                #print(pointer)\n",
    "                img_offset = pointer*sector_size\n",
    "                img_h.seek(img_offset)\n",
    "                fsector = img_h.read(sector_size)\n",
    "                sector_md5 = hashlib.md5(fsector).hexdigest()\n",
    "                sector_hash_list.append((img_offset,sector_md5))\n",
    "    return sector_hash_list\n",
    "\n",
    "def process_object_sectors(range_tuple):\n",
    "    sector_size = 512\n",
    "    img_csv = \"/Volumes/Samsung_T5/argo/pat2.csv\"\n",
    "    img_csv_df = pd.read_csv(img_csv)\n",
    "    global options\n",
    "    files_cols = ['obj_id', 'partition','inode','filename','filesize']\n",
    "    block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "    file_df = pd.DataFrame(columns=files_cols)\n",
    "    ebyterun_df = pd.DataFrame(columns=block_hashes_cols)\n",
    "    return_list = []\n",
    "    for obj in range(range_tuple[0], range_tuple[1]):\n",
    "        # Filter out specific filenames create by TSK that are not of use\n",
    "        print(\"processing this  %s\" % str(obj))\n",
    "        print(type(obj))\n",
    "        #if int(obj._tags['id']) == id:# and obj.filesize() == False:\n",
    "        #counter += 1\n",
    "        #print(obj.filename())\n",
    "        data = [str(obj), obj.partition(),obj.inode(), obj.filename(),  obj.filesize()] #obj._tags['id']\n",
    "        file_df.loc[len(file_df.index)] = data\n",
    "        print(data)\n",
    "        byterun = []\n",
    "        persist_img_offset = 0\n",
    "        persist_fs_offset = 0\n",
    "        persist_file_offset = 0\n",
    "        remaining_len = obj.filesize()\n",
    "        #ebyterun_df = pd.DataFrame(columns=['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1'])\n",
    "        for ebyterun in obj.byte_runs():\n",
    "            #pprint(vars(ebyterun)) \n",
    "            #byterun.append([ebyterun.img_offset,ebyterun.file_offset, ebyterun.len, ebyterun.fs_offset])\n",
    "            byterun.append(ebyterun.file_offset)\n",
    "            persist_file_offset = ebyterun.file_offset\n",
    "            byterun.append(ebyterun.len)\n",
    "            if hasattr(ebyterun,'img_offset'): \n",
    "                byterun.append(ebyterun.img_offset)\n",
    "                if ebyterun.img_offset != None: persist_img_offset = ebyterun.img_offset\n",
    "            if hasattr(ebyterun,'fs_offset'): \n",
    "                byterun.append(ebyterun.fs_offset)\n",
    "                if ebyterun.fs_offset != None: persist_fs_offset = ebyterun.fs_offset\n",
    "            if hasattr(ebyterun,'len'): \n",
    "                byterun.append(ebyterun.len)\n",
    "                if ebyterun.len != None: persist_len = ebyterun.len\n",
    "            #print(\"img_offset\", img_offset, \"file offset\", ebyterun.file_offset)\n",
    "            #print(\"file_offset\", \"len\", \"img_offset\", \"fs_offset\")\n",
    "            byterun_start = int(persist_img_offset / sector_size)\n",
    "            byterun_end = int((math.ceil((persist_img_offset + ebyterun.len) / sector_size))) - 1\n",
    "            len_run = np.arange(persist_len,0,-sector_size) #remaining_len\n",
    "            sector_run = np.full(len(len_run), sector_size)\n",
    "            len_run = np.minimum(len_run,sector_run)\n",
    "            #len_run = min(sector_size,len_run.all())\n",
    "            #print(\"len_run\",len(len_run), len(len_run)*sector_size)\n",
    "            #print(\"byterun_start\", byterun_start,persist_img_offset, \"byterun_end\", byterun_end,  \"len\", ebyterun.len)\n",
    "            #print(\"ebyterun_df\", ebyterun_df.shape, \"img_csv\", img_csv_df.loc[byterun_start:byterun_end,'img_offset'].shape )\n",
    "            #print(img_csv_df.loc[byterun_start:byterun_end,:])\n",
    "            #print(\"img_csv_len\",len(img_csv_df.loc[byterun_start:byterun_end, :]))\n",
    "            ebyterun_df.loc[:,'img_sector_offset'] = np.arange(len(img_csv_df))\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'obj_id'] = str(obj) #obj._tags['id']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'img_offset'] = img_csv_df.loc[byterun_start:byterun_end,'img_offset']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'fs_offset'] = np.arange(persist_fs_offset,persist_fs_offset+ebyterun.len,sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'file_offset'] = np.arange(persist_file_offset,persist_file_offset+ebyterun.len, sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'len'] = len_run\n",
    "            #print(\"remaining_len\",remaining_len)\n",
    "            #remaining_len-=sector_size\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'md5'] = img_csv_df.loc[byterun_start:byterun_end,'md5']\n",
    "            return_list.append((file_df,ebyterun_df))\n",
    "        print(\"done processing %s\" % str(obj))\n",
    "    return return_list   \n",
    "\n",
    "def determine_subranges(fullrange, num_subranges):\n",
    "    \"\"\"\n",
    "    Break fullrange up into smaller sets of ranges that cover all\n",
    "    the same numbers.\n",
    "    \"\"\"\n",
    "    subranges = []\n",
    "    inc = fullrange[1] // num_subranges\n",
    "    for i in range(fullrange[0], fullrange[1], inc):\n",
    "        subranges.append( (i, min(i+inc, fullrange[1])) )\n",
    "    return( subranges )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #sector_size = 512\n",
    "    img_path = '/Volumes/Samsung_T5/M57/pat-2009-12-10.raw.xml'\n",
    "    object_number = dfxml.iter_dfxml(img_path, preserve_elements=True)\n",
    "    img_objcount = len(list(object_number))\n",
    "    fullrange = (0, img_objcount)\n",
    "    num_subranges = 1000\n",
    "    subranges = determine_subranges(fullrange, num_subranges)\n",
    "\n",
    "    executor = MPIPoolExecutor()\n",
    "    sectors_df_list = executor.map(process_object_sectors, subranges)\n",
    "    #files_df_list = executor.map(process_object_files, subranges)\n",
    "    executor.shutdown()\n",
    "    grand_sectors_df = pd.DataFrame()\n",
    "    grand_files_df = pd.DataFrame()\n",
    "    # flatten the list of lists\n",
    "    for df in sectors_df_list:\n",
    "        grand_files_df = pd.concat([grand_files_df, df[0]])\n",
    "        grand_sectors_df = pd.concat([grand_sectors_df, df[1]])\n",
    "    #for df in files_df_list:\n",
    "    #    grand_files_df = pd.concat([grand_files_df, df])\n",
    "    #print(textwrap.fill(str(primes),80))\n",
    "    con = sqlite3.connect(\"/Volumes/Samsung_T5/M57/pat.db\")\n",
    "    grand_sectors_df.to_sql('block_hashes', con, index=True)\n",
    "    grand_files_df.to_sql('files', con, index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/oadegbeh/M57/pat-2009-12-10.raw.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/seunfuta/Downloads/paper2research/argo/test.ipynb Cell 26\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/seunfuta/Downloads/paper2research/argo/test.ipynb#X26sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m object_gen \u001b[39m=\u001b[39m dfxml\u001b[39m.\u001b[39miter_dfxml(img_path, preserve_elements\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/seunfuta/Downloads/paper2research/argo/test.ipynb#X26sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m obj_list \u001b[39m=\u001b[39m []\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/seunfuta/Downloads/paper2research/argo/test.ipynb#X26sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mfor\u001b[39;00m each_obj \u001b[39min\u001b[39;00m object_gen:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/seunfuta/Downloads/paper2research/argo/test.ipynb#X26sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     obj_list\u001b[39m.\u001b[39mappend(each_obj)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/seunfuta/Downloads/paper2research/argo/test.ipynb#X26sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39m#img_objcount = len(obj_list)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/seunfuta/Downloads/paper2research/argo/test.ipynb#X26sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39m#fullrange = (0, img_objcount)\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/paper2research/argo/dfxml/__init__.py:1530\u001b[0m, in \u001b[0;36miter_dfxml\u001b[0;34m(xmlfile, preserve_elements, imagefile)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mxmlfile must be specified\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1529\u001b[0m qtagname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m}fileobject\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m XMLNS_DFXML\n\u001b[0;32m-> 1530\u001b[0m \u001b[39mfor\u001b[39;00m event, elem \u001b[39min\u001b[39;00m ET\u001b[39m.\u001b[39;49miterparse(xmlfile, (\u001b[39m\"\u001b[39;49m\u001b[39mstart\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m)):\n\u001b[1;32m   1531\u001b[0m     \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1532\u001b[0m         \u001b[39m#Note that ElementTree qualifies tag names if possible.  Thus, the paired check.\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m         \u001b[39mif\u001b[39;00m elem\u001b[39m.\u001b[39mtag \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mfileobject\u001b[39m\u001b[39m\"\u001b[39m, qtagname]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/scan_match_validate_all_mpi/lib/python3.8/xml/etree/ElementTree.py:1248\u001b[0m, in \u001b[0;36miterparse\u001b[0;34m(source, events, parser)\u001b[0m\n\u001b[1;32m   1246\u001b[0m close_source \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(source, \u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1248\u001b[0m     source \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(source, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1249\u001b[0m     close_source \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m \u001b[39mreturn\u001b[39;00m it\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/oadegbeh/M57/pat-2009-12-10.raw.xml'"
     ]
    }
   ],
   "source": [
    "#LATEST VERSION\n",
    "# MPIpool.py\n",
    "\n",
    "from mpi4py.futures import MPIPoolExecutor\n",
    "import math\n",
    "import textwrap\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import repeat\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pprint\n",
    "from pprint import pprint\n",
    "import dfxml \n",
    "\n",
    "def compute_hash(range_tuple):\n",
    "    img_path = '/home/oadegbeh/M57/pat-2009-12-10.raw.raw'\n",
    "    sector_hash_list = []\n",
    "    print(range_tuple[0], range_tuple[1])\n",
    "    sector_size = 512\n",
    "    img_h = open(img_path, 'rb')\n",
    "    for pointer in range(range_tuple[0], range_tuple[1]):\n",
    "                #print(pointer)\n",
    "                img_offset = pointer*sector_size\n",
    "                img_h.seek(img_offset)\n",
    "                fsector = img_h.read(sector_size)\n",
    "                sector_md5 = hashlib.md5(fsector).hexdigest()\n",
    "                sector_hash_list.append((img_offset,sector_md5))\n",
    "    return sector_hash_list\n",
    "\n",
    "def process_object_sectors(objlist):\n",
    "    sector_size = 512\n",
    "    img_csv = \"/home/oadegbeh/M57/pat2.csv\"\n",
    "    img_csv_df = pd.read_csv(img_csv)\n",
    "    global options\n",
    "    files_cols = ['obj_id', 'partition','inode','filename','filesize']\n",
    "    block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "    file_df = pd.DataFrame(columns=files_cols)\n",
    "    ebyterun_df = pd.DataFrame(columns=block_hashes_cols)\n",
    "    return_list = []\n",
    "    for obj in objlist:#range(range_tuple[0], range_tuple[1]):\n",
    "        # Filter out specific filenames create by TSK that are not of use\n",
    "        print(\"processing %s\" % obj._tags['id']) #str(obj)\n",
    "        #print(type(obj))\n",
    "        #if int(obj._tags['id']) == id:# and obj.filesize() == False:\n",
    "        #counter += 1\n",
    "        #print(obj.filename())\n",
    "        data = [obj._tags['id'], obj.partition(),obj.inode(), obj.filename(),  obj.filesize()] #\n",
    "        file_df.loc[len(file_df.index)] = data\n",
    "        print(data)\n",
    "        byterun = []\n",
    "        persist_img_offset = 0\n",
    "        persist_fs_offset = 0\n",
    "        persist_file_offset = 0\n",
    "        remaining_len = obj.filesize()\n",
    "        #ebyterun_df = pd.DataFrame(columns=['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1'])\n",
    "        for ebyterun in obj.byte_runs():\n",
    "            #pprint(vars(ebyterun)) \n",
    "            #byterun.append([ebyterun.img_offset,ebyterun.file_offset, ebyterun.len, ebyterun.fs_offset])\n",
    "            byterun.append(ebyterun.file_offset)\n",
    "            persist_file_offset = ebyterun.file_offset\n",
    "            byterun.append(ebyterun.len)\n",
    "            if hasattr(ebyterun,'img_offset'): \n",
    "                byterun.append(ebyterun.img_offset)\n",
    "                if ebyterun.img_offset != None: persist_img_offset = ebyterun.img_offset\n",
    "            if hasattr(ebyterun,'fs_offset'): \n",
    "                byterun.append(ebyterun.fs_offset)\n",
    "                if ebyterun.fs_offset != None: persist_fs_offset = ebyterun.fs_offset\n",
    "            if hasattr(ebyterun,'len'): \n",
    "                byterun.append(ebyterun.len)\n",
    "                if ebyterun.len != None: persislen = ebyterun.len\n",
    "            #print(\"img_offset\", img_offset, \"file offset\", ebyterun.file_offset)\n",
    "            #print(\"file_offset\", \"len\", \"img_offset\", \"fs_offset\")\n",
    "            byterun_start = int(persist_img_offset / sector_size)\n",
    "            byterun_end = int((math.ceil((persist_img_offset + ebyterun.len) / sector_size))) - 1\n",
    "            len_run = np.arange(persist_len,0,-sector_size) #remaining_len\n",
    "            sector_run = np.full(len(len_run), sector_size)\n",
    "            len_run = np.minimum(len_run,sector_run)\n",
    "            #len_run = min(sector_size,len_run.all())\n",
    "            #print(\"len_run\",len(len_run), len(len_run)*sector_size)\n",
    "            #print(\"byterun_start\", byterun_start,persist_img_offset, \"byterun_end\", byterun_end,  \"len\", ebyterun.len)\n",
    "            #print(\"ebyterun_df\", ebyterun_df.shape, \"img_csv\", img_csv_df.loc[byterun_start:byterun_end,'img_offset'].shape )\n",
    "            #print(img_csv_df.loc[byterun_start:byterun_end,:])\n",
    "            #print(\"img_csv_len\",len(img_csv_df.loc[byterun_start:byterun_end, :]))\n",
    "            ebyterun_df.loc[:,'img_sector_offset'] = np.arange(len(img_csv_df))\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'obj_id'] = str(obj) #obj._tags['id']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'img_offset'] = img_csv_df.loc[byterun_start:byterun_end,'img_offset']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'fs_offset'] = np.arange(persist_fs_offset,persist_fs_offset+ebyterun.len,sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'file_offset'] = np.arange(persist_file_offset,persist_file_offset+ebyterun.len, sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'len'] = len_run\n",
    "            #print(\"remaining_len\",remaining_len)\n",
    "            #remaining_len-=sector_size\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'md5'] = img_csv_df.loc[byterun_start:byterun_end,'md5']\n",
    "            return_list.append((file_df,ebyterun_df))\n",
    "        print(\"done processing %s\" % obj._tags['id']) #str(obj)\n",
    "    return return_list   \n",
    "\n",
    "def determine_subranges(obj_list, num_subranges):#fullrange\n",
    "    \"\"\"\n",
    "    Break fullrange up into smaller sets of ranges that cover all\n",
    "    the same numbers.\n",
    "    \"\"\"\n",
    "    subranges = []\n",
    "    inc =  len(obj_list)// num_subranges #fullrange[1]\n",
    "    for i in range(0, len(obj_list), inc):#fullrange[0], fullrange[1]\n",
    "        subranges.append( (obj_list[i], obj_list[min(i+inc,len(obj_list)-1 )]) )#fullrange[1]\n",
    "    return( subranges )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #sector_size = 512\n",
    "    img_path = '/home/oadegbeh/M57/pat-2009-12-10.raw.xml'\n",
    "    object_gen = dfxml.iter_dfxml(img_path, preserve_elements=True)\n",
    "    obj_list = []\n",
    "    for each_obj in object_gen:\n",
    "        obj_list.append(each_obj)\n",
    "    #img_objcount = len(obj_list)\n",
    "    #fullrange = (0, img_objcount)\n",
    "    num_subranges = 1000\n",
    "    subranges = determine_subranges(obj_list, num_subranges)#fullrange\n",
    "\n",
    "    executor = MPIPoolExecutor()\n",
    "    sectors_df_list = executor.map(process_object_sectors, subranges)\n",
    "    #files_df_list = executor.map(process_object_files, subranges)\n",
    "    executor.shutdown()\n",
    "    grand_sectors_df = pd.DataFrame()\n",
    "    grand_files_df = pd.DataFrame()\n",
    "    # flatten the list of lists\n",
    "    for df in sectors_df_list: #COULD THIS BE A PROBLEM POINT?\n",
    "        grand_files_df = pd.concat([grand_files_df, df[0]])\n",
    "        grand_sectors_df = pd.concat([grand_sectors_df, df[1]])\n",
    "    #for df in files_df_list:\n",
    "    #    grand_files_df = pd.concat([grand_files_df, df])\n",
    "    #print(textwrap.fill(str(primes),80))\n",
    "    con = sqlite3.connect(\"/scratch/oadegbeh/pat.db\")\n",
    "    grand_sectors_df.to_sql('block_hashes', con, index=True)\n",
    "    grand_files_df.to_sql('files', con, index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   obj_id   img_offset    fs_offset file_offset  len  md5 sha1\n",
      "0   15304   6259150336   6259118080           0  512  NaN  NaN\n",
      "1   15304   6259150848   6259118592         512  421  NaN  NaN\n",
      "0   15308   5200887296   5200855040           0  512  NaN  NaN\n",
      "1   15308   5200887808   5200855552         512  512  NaN  NaN\n",
      "2   15308   5200888320   5200856064        1024  512  NaN  NaN\n",
      "..    ...          ...          ...         ...  ...  ...  ...\n",
      "9   16060  10176974848  10176942592       20992  512  NaN  NaN\n",
      "10  16060  10176975360  10176943104       21504  512  NaN  NaN\n",
      "11  16060  10176975872  10176943616       22016  512  NaN  NaN\n",
      "12  16060  10176976384  10176944128       22528  512  NaN  NaN\n",
      "13  16060  10176976896  10176944640       23040  236  NaN  NaN\n",
      "\n",
      "[557458 rows x 7 columns]\n",
      "   obj_id partition  inode                                           filename  \\\n",
      "0   15304         1  10633            dell/drivers/R54402/Unattend/Readme.txt   \n",
      "0   15308         1  10635     dell/drivers/R54402/Unattend/Win2K/Pushw2k.txt   \n",
      "0   15309         1  10636    dell/drivers/R54402/Unattend/Win2K/Unattend.txt   \n",
      "0   15314         1  10639     dell/drivers/R54402/Unattend/Win98/MSBATCH.INF   \n",
      "0   15315         1  10640     dell/drivers/R54402/Unattend/Win98/PUSHW98.TXT   \n",
      "..    ...       ...    ...                                                ...   \n",
      "0   16055         1  12206  Program Files/AVG/AVG9/Icons/click_here_yellow...   \n",
      "0   16056         1  12129             Program Files/AVG/AVG9/Icons/clock.gif   \n",
      "0   16058         1  12208       Program Files/AVG/AVG9/Icons/icons_close.gif   \n",
      "0   16059         1  12536                 Program Files/AVG/AVG9/libsasl.dll   \n",
      "0   16060         1  12715              Program Files/AVG/AVG9/license_us.htm   \n",
      "\n",
      "   filesize  \n",
      "0       933  \n",
      "0      6650  \n",
      "0       860  \n",
      "0     19090  \n",
      "0      3961  \n",
      "..      ...  \n",
      "0      1368  \n",
      "0      2455  \n",
      "0       613  \n",
      "0     53528  \n",
      "0     23276  \n",
      "\n",
      "[443 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# MPIpool.py\n",
    "\n",
    "from mpi4py.futures import MPIPoolExecutor\n",
    "import math\n",
    "import textwrap\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import repeat\n",
    "#import sqlite3\n",
    "import numpy as np\n",
    "import pprint\n",
    "from pprint import pprint\n",
    "import dfxml \n",
    "\n",
    "def compute_hash(range_tuple):\n",
    "    img_path = '/home/oadegbeh/M57/pat-2009-12-10.raw.raw'\n",
    "    sector_hash_list = []\n",
    "    print(range_tuple[0], range_tuple[1])\n",
    "    sector_size = 512\n",
    "    img_h = open(img_path, 'rb')\n",
    "    for pointer in range(range_tuple[0], range_tuple[1]):\n",
    "                #print(pointer)\n",
    "                img_offset = pointer*sector_size\n",
    "                img_h.seek(img_offset)\n",
    "                fsector = img_h.read(sector_size)\n",
    "                sector_md5 = hashlib.md5(fsector).hexdigest()\n",
    "                sector_hash_list.append((img_offset,sector_md5))\n",
    "    return sector_hash_list\n",
    "\n",
    "def generatorY(subrange):\n",
    "    #print(\"len of subrange\", len(subrange))\n",
    "    counter = 0\n",
    "    for obj in subrange:\n",
    "        #print(\"counter\", counter)\n",
    "        counter +=1\n",
    "        #print(obj.filename())\n",
    "        block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "        block_hash_df = pd.DataFrame(columns=block_hashes_cols)\n",
    "        \n",
    "        p_img_offset = 0\n",
    "        p_fs_offset = 0\n",
    "        p_file_offset = 0\n",
    "        p_len = 0#\n",
    "        sector_size = 512\n",
    "        for byterun in obj.byte_runs():\n",
    "            c = vars(byterun)\n",
    "            #print(c)\n",
    "            #print(\"has type?\", str(hasattr(byterun,'type')))\n",
    "            if (hasattr(byterun,'type') and c['type']== 'resident'):\n",
    "                #print(\"resident!\")\n",
    "                continue\n",
    "            else:\n",
    "                #print(\"not resident\")\n",
    "                p_img_offset = c['img_offset'] if (hasattr(byterun,'img_offset') and byterun.img_offset != None) else p_img_offset\n",
    "                #print(\"image offset\", p_img_offset)\n",
    "                p_fs_offset = c['fs_offset'] if (hasattr(byterun,'fs_offset') and byterun.fs_offset != None) else p_fs_offset\n",
    "                #print(\"fs_offset\", p_fs_offset)\n",
    "                p_file_offset = c['file_offset'] if (hasattr(byterun,'file_offset') and byterun.file_offset != None) else p_file_offset\n",
    "                #print(\"file_offset\",p_file_offset)\n",
    "                p_len = byterun.len if (hasattr(byterun,'len') and byterun.len != None) else byterun.uncompressed_len if (hasattr(byterun,'uncompressed_len')and byterun.uncompressed_len != None) else p_len\n",
    "                #print(\"len\", p_len)\n",
    "                img_start = p_img_offset//sector_size\n",
    "                img_stop = (p_img_offset+max(p_len,sector_size))//sector_size #(p_img_offset+max(p_len, sector_size))//sector_size\n",
    "                fs_start = p_fs_offset//sector_size\n",
    "                fs_stop = (p_fs_offset+max(p_len,sector_size))//sector_size\n",
    "                file_start = p_file_offset//sector_size\n",
    "                file_stop = (p_file_offset+max(p_len,sector_size))//sector_size\n",
    "                #print(\"start\", start, \"stop\", stop, )\n",
    "                cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "                df = pd.DataFrame(columns=cols)\n",
    "                #df.loc[:,'img_sector_offset'] = np.arange(img_start, img_stop)\n",
    "                #print(\"df length\", len(df))\n",
    "                #print(\"img offset len\", len(np.arange(p_img_offset,p_img_offset+p_len,sector_size)))\n",
    "                #print(\"fs_offset len\", len(np.arange(p_fs_offset,p_fs_offset+p_len,sector_size)))\n",
    "                #print(\"file offset len\", len(np.arange(p_file_offset,p_file_offset+p_len, sector_size)))\n",
    "                #print(\"img_offset\", p_img_offset, \"fs_offset\", p_fs_offset,\"file_offset\", p_file_offset, \"p_len\", p_len)\n",
    "                df.loc[:,'obj_id'] = obj._tags['id'] #obj._tags['id']\n",
    "                #print(\"obj._tags['id']\", obj._tags['id'])\n",
    "                df.loc[:,'img_offset'] = np.arange(p_img_offset,p_img_offset+p_len,sector_size)\n",
    "                df.loc[:,'fs_offset'] = np.arange(p_fs_offset,p_fs_offset+p_len,sector_size)\n",
    "                df.loc[:,'file_offset'] = np.arange(p_file_offset,p_file_offset+p_len, sector_size)\n",
    "                df.loc[:,'obj_id'] = obj._tags['id'] #obj._tags['id']\n",
    "                #print(\"obj._tags['id']\", obj._tags['id'])\n",
    "                len_run = np.arange(p_len,0,-sector_size) #remaining_len\n",
    "                sector_run = np.full(len(len_run), sector_size)\n",
    "                len_run = np.minimum(len_run,sector_run)\n",
    "                #print(\"len run\", len(len_run))\n",
    "                #print(len_run)\n",
    "                df.loc[:,'len'] = len_run\n",
    "                block_hash_df = pd.concat([block_hash_df, df])\n",
    "                p_img_offset += p_len\n",
    "                p_fs_offset += p_len\n",
    "                p_file_offset += p_len\n",
    "                #print(\"img_offsetE\", p_img_offset, \"fs_offset\", p_fs_offset, \"file_offset\", p_file_offset)\n",
    "                #block_hash_df = pd.concat([block_hash_df, df])\n",
    "\n",
    "        if len(block_hash_df) != 0:\n",
    "            files_cols = ['obj_id', 'partition','inode','filename','filesize']\n",
    "            file_df = pd.DataFrame(columns=files_cols)\n",
    "            file_df.loc[len(file_df),:] = [obj._tags['id'],obj.partition(), obj.inode(), obj.filename(), obj.filesize()]\n",
    "            yield file_df, block_hash_df\n",
    "\n",
    "        \n",
    "\n",
    "def process_objects(subrange):\n",
    "    #print(\"subrange\", type(subrange), subrange)\n",
    "    combo_files_df = pd.DataFrame()\n",
    "    combo_sectors_df = pd.DataFrame()\n",
    "    for files_df,sectors_df in generatorY(subrange):\n",
    "        combo_files_df = pd.concat([combo_files_df, files_df])\n",
    "        combo_sectors_df = pd.concat([combo_sectors_df, sectors_df])\n",
    "    \n",
    "    return combo_files_df, combo_sectors_df\n",
    "\n",
    "def process_object_sectors(objlist):\n",
    "    sector_size = 512\n",
    "    img_csv = \"/home/oadegbeh/M57/pat2.csv\"\n",
    "    img_csv_df = pd.read_csv(img_csv)\n",
    "    global options\n",
    "    files_cols = ['obj_id', 'partition','inode','filename','filesize']\n",
    "    block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "    file_df = pd.DataFrame(columns=files_cols)\n",
    "    ebyterun_df = pd.DataFrame(columns=block_hashes_cols)\n",
    "    return_list = []\n",
    "    for obj in objlist:#range(range_tuple[0], range_tuple[1]):\n",
    "        # Filter out specific filenames create by TSK that are not of use\n",
    "        print(\"processing %s\" % obj._tags['id']) #str(obj)\n",
    "        #print(type(obj))\n",
    "        #if int(obj._tags['id']) == id:# and obj.filesize() == False:\n",
    "        #counter += 1\n",
    "        print(obj.filename(), obj.filesize())\n",
    "        data = [obj._tags['id'], obj.partition(),obj.inode(), obj.filename(),  obj.filesize()] #\n",
    "        file_df.loc[len(file_df.index)] = data\n",
    "        print(data)\n",
    "        byterun = []\n",
    "        persist_img_offset = 0\n",
    "        persist_fs_offset = 0\n",
    "        persist_file_offset = 0\n",
    "        remaining_len = obj.filesize()\n",
    "        #ebyterun_df = pd.DataFrame(columns=['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1'])\n",
    "        for ebyterun in obj.byte_runs():\n",
    "            #pprint(vars(ebyterun)) \n",
    "            #byterun.append([ebyterun.img_offset,ebyterun.file_offset, ebyterun.len, ebyterun.fs_offset])\n",
    "            byterun.append(ebyterun.file_offset)\n",
    "            persist_file_offset = ebyterun.file_offset\n",
    "            byterun.append(ebyterun.len)\n",
    "            if hasattr(ebyterun,'img_offset'): \n",
    "                byterun.append(ebyterun.img_offset)\n",
    "                if ebyterun.img_offset != None: persist_img_offset = ebyterun.img_offset\n",
    "            if hasattr(ebyterun,'fs_offset'): \n",
    "                byterun.append(ebyterun.fs_offset)\n",
    "                if ebyterun.fs_offset != None: persist_fs_offset = ebyterun.fs_offset\n",
    "            if hasattr(ebyterun,'len'): \n",
    "                byterun.append(ebyterun.len)\n",
    "                if ebyterun.len != None: persist_len = ebyterun.len\n",
    "            #print(\"img_offset\", img_offset, \"file offset\", ebyterun.file_offset)\n",
    "            #print(\"file_offset\", \"len\", \"img_offset\", \"fs_offset\")\n",
    "            byterun_start = int(persist_img_offset / sector_size)\n",
    "            byterun_end = int((math.ceil((persist_img_offset + ebyterun.len) / sector_size))) - 1\n",
    "            len_run = np.arange(persist_len,0,-sector_size) #remaining_len\n",
    "            sector_run = np.full(len(len_run), sector_size)\n",
    "            len_run = np.minimum(len_run,sector_run)\n",
    "            #len_run = min(sector_size,len_run.all())\n",
    "            #print(\"len_run\",len(len_run), len(len_run)*sector_size)\n",
    "            #print(\"byterun_start\", byterun_start,persist_img_offset, \"byterun_end\", byterun_end,  \"len\", ebyterun.len)\n",
    "            #print(\"ebyterun_df\", ebyterun_df.shape, \"img_csv\", img_csv_df.loc[byterun_start:byterun_end,'img_offset'].shape )\n",
    "            #print(img_csv_df.loc[byterun_start:byterun_end,:])\n",
    "            #print(\"img_csv_len\",len(img_csv_df.loc[byterun_start:byterun_end, :]))\n",
    "            ebyterun_df.loc[:,'img_sector_offset'] = np.arange(len(img_csv_df))\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'obj_id'] = str(obj) #obj._tags['id']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'img_offset'] = img_csv_df.loc[byterun_start:byterun_end,'img_offset']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'fs_offset'] = np.arange(persist_fs_offset,persist_fs_offset+ebyterun.len,sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'file_offset'] = np.arange(persist_file_offset,persist_file_offset+ebyterun.len, sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'len'] = len_run\n",
    "            #print(\"remaining_len\",remaining_len)\n",
    "            #remaining_len-=sector_size\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'md5'] = img_csv_df.loc[byterun_start:byterun_end,'md5']\n",
    "            return_list.append((file_df,ebyterun_df))\n",
    "        print(\"done processing %s\" % obj._tags['id']) #str(obj)\n",
    "    return return_list   \n",
    "\n",
    "def determine_subranges(obj_list, num_subranges):#fullrange\n",
    "    \"\"\"\n",
    "    Break fullrange up into smaller sets of ranges that cover all\n",
    "    the same numbers.\n",
    "    \"\"\"\n",
    "    subranges = []\n",
    "    inc =  len(obj_list)// num_subranges #fullrange[1]\n",
    "    #print(\"inc\", inc)\n",
    "    for i in range(0, len(obj_list), inc):#fullrange[0], fullrange[1]\n",
    "        #print(i,min(i+inc,len(obj_list)))\n",
    "        subranges.append( obj_list[i:min(i+inc,len(obj_list) )] )#fullrange[1]\n",
    "    return subranges \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #sector_size = 512\n",
    "    img_path = '/Volumes/Samsung_T5/M57/pat-2009-12-10.raw.xml'\n",
    "    object_gen = dfxml.iter_dfxml(img_path, preserve_elements=True)\n",
    "    obj_list = []\n",
    "    for each_obj in object_gen:\n",
    "        obj_list.append(each_obj)\n",
    "    #img_objcount = len(obj_list)\n",
    "    #fullrange = (0, img_objcount)\n",
    "    #print(\"obj list\",len(obj_list))\n",
    "    num_subranges = 65\n",
    "    subranges = determine_subranges(obj_list, num_subranges)#fullrange\n",
    "    #for r in subranges:\n",
    "    #    print(\"mini subrange size\", len(list(r)))\n",
    "    #print(\"subranges len\", type(subranges), len(list(subranges)) )\n",
    "    executor = MPIPoolExecutor()\n",
    "    #print(\"subranges[20]\", len(subranges[20]))\n",
    "    sectors_df_list = process_objects(subranges[20])\n",
    "    #sectors_df_list = executor.map(process_objects, subranges)\n",
    "    #files_df_list = executor.map(process_object_files, subranges)\n",
    "    executor.shutdown()\n",
    "    #grand_sectors_df = pd.DataFrame()\n",
    "    #grand_files_df = pd.DataFrame()\n",
    "    # flatten the list of lists\n",
    "    #print(\"sectors_df_list\", type(sectors_df_list))\n",
    "    \n",
    "    #print(\"df \", type(df), len(df))\n",
    "    #dataframe1 = pd.DataFrame(df[0])\n",
    "    #dataframe2 = pd.DataFrame(df[1])\n",
    "    #print(\"dataframe1\", dataframe1)\n",
    "    #print(\"dataframe2\", dataframe2)\n",
    "\n",
    "    #print(\"df[1]\", df[1])\n",
    "    grand_files_df, grand_sectors_df = sectors_df_list\n",
    "    #grand_files_df = pd.concat([grand_files_df, df[0]])\n",
    "    #grand_sectors_df = pd.concat([grand_sectors_df, df[1]])\n",
    "    #for df in files_df_list:\n",
    "    #    grand_files_df = pd.concat([grand_files_df, df])\n",
    "    #print(textwrap.fill(str(primes),80))\n",
    "    print(grand_sectors_df)\n",
    "    print(grand_files_df)\n",
    "    '''\n",
    "    con = sqlite3.connect(\"/Volumes/Samsung_T5/M57/pat.db\")\n",
    "    grand_sectors_df.to_sql('block_hashes', con, index=True)\n",
    "    grand_files_df.to_sql('files', con, index=True)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   obj_id partition  inode                                           filename  \\\n",
      "0    7651         1  41412  Documents and Settings/Pat/Local Settings/Temp...   \n",
      "0    7654         1  34985  Documents and Settings/Pat/Local Settings/Temp...   \n",
      "0    7655         1  28997  Documents and Settings/Pat/Local Settings/Temp...   \n",
      "0    7656         1  14167  Documents and Settings/Pat/Local Settings/Temp...   \n",
      "0    7657         1  32975  Documents and Settings/Pat/Local Settings/Temp...   \n",
      "..    ...       ...    ...                                                ...   \n",
      "0    8409         1  35163  Documents and Settings/Pat/Local Settings/Temp...   \n",
      "0    8410         1  36227  Documents and Settings/Pat/Local Settings/Temp...   \n",
      "0    8411         1  23722  Documents and Settings/Pat/Local Settings/Temp...   \n",
      "0    8414         1  34525  Documents and Settings/Pat/Local Settings/Temp...   \n",
      "0    8415         1  14244  Documents and Settings/Pat/Local Settings/Temp...   \n",
      "\n",
      "   filesize  \n",
      "0      2663  \n",
      "0      4522  \n",
      "0     13140  \n",
      "0     29607  \n",
      "0       858  \n",
      "..      ...  \n",
      "0     10189  \n",
      "0      2652  \n",
      "0      3169  \n",
      "0      2394  \n",
      "0      1482  \n",
      "\n",
      "[512 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(grand_sectors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_files_df.loc[:,'Q'] =  grand_files_df.filesize //512\n",
    "grand_files_df.loc[:,'r'] =  grand_files_df.filesize % 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_id</th>\n",
       "      <th>partition</th>\n",
       "      <th>inode</th>\n",
       "      <th>filename</th>\n",
       "      <th>filesize</th>\n",
       "      <th>Q</th>\n",
       "      <th>r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7651</td>\n",
       "      <td>1</td>\n",
       "      <td>41412</td>\n",
       "      <td>Documents and Settings/Pat/Local Settings/Temp...</td>\n",
       "      <td>2663</td>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7654</td>\n",
       "      <td>1</td>\n",
       "      <td>34985</td>\n",
       "      <td>Documents and Settings/Pat/Local Settings/Temp...</td>\n",
       "      <td>4522</td>\n",
       "      <td>8</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7655</td>\n",
       "      <td>1</td>\n",
       "      <td>28997</td>\n",
       "      <td>Documents and Settings/Pat/Local Settings/Temp...</td>\n",
       "      <td>13140</td>\n",
       "      <td>25</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7656</td>\n",
       "      <td>1</td>\n",
       "      <td>14167</td>\n",
       "      <td>Documents and Settings/Pat/Local Settings/Temp...</td>\n",
       "      <td>29607</td>\n",
       "      <td>57</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7657</td>\n",
       "      <td>1</td>\n",
       "      <td>32975</td>\n",
       "      <td>Documents and Settings/Pat/Local Settings/Temp...</td>\n",
       "      <td>858</td>\n",
       "      <td>1</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8409</td>\n",
       "      <td>1</td>\n",
       "      <td>35163</td>\n",
       "      <td>Documents and Settings/Pat/Local Settings/Temp...</td>\n",
       "      <td>10189</td>\n",
       "      <td>19</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8410</td>\n",
       "      <td>1</td>\n",
       "      <td>36227</td>\n",
       "      <td>Documents and Settings/Pat/Local Settings/Temp...</td>\n",
       "      <td>2652</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8411</td>\n",
       "      <td>1</td>\n",
       "      <td>23722</td>\n",
       "      <td>Documents and Settings/Pat/Local Settings/Temp...</td>\n",
       "      <td>3169</td>\n",
       "      <td>6</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8414</td>\n",
       "      <td>1</td>\n",
       "      <td>34525</td>\n",
       "      <td>Documents and Settings/Pat/Local Settings/Temp...</td>\n",
       "      <td>2394</td>\n",
       "      <td>4</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8415</td>\n",
       "      <td>1</td>\n",
       "      <td>14244</td>\n",
       "      <td>Documents and Settings/Pat/Local Settings/Temp...</td>\n",
       "      <td>1482</td>\n",
       "      <td>2</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   obj_id partition  inode                                           filename  \\\n",
       "0    7651         1  41412  Documents and Settings/Pat/Local Settings/Temp...   \n",
       "0    7654         1  34985  Documents and Settings/Pat/Local Settings/Temp...   \n",
       "0    7655         1  28997  Documents and Settings/Pat/Local Settings/Temp...   \n",
       "0    7656         1  14167  Documents and Settings/Pat/Local Settings/Temp...   \n",
       "0    7657         1  32975  Documents and Settings/Pat/Local Settings/Temp...   \n",
       "..    ...       ...    ...                                                ...   \n",
       "0    8409         1  35163  Documents and Settings/Pat/Local Settings/Temp...   \n",
       "0    8410         1  36227  Documents and Settings/Pat/Local Settings/Temp...   \n",
       "0    8411         1  23722  Documents and Settings/Pat/Local Settings/Temp...   \n",
       "0    8414         1  34525  Documents and Settings/Pat/Local Settings/Temp...   \n",
       "0    8415         1  14244  Documents and Settings/Pat/Local Settings/Temp...   \n",
       "\n",
       "   filesize   Q    r  \n",
       "0      2663   5  103  \n",
       "0      4522   8  426  \n",
       "0     13140  25  340  \n",
       "0     29607  57  423  \n",
       "0       858   1  346  \n",
       "..      ...  ..  ...  \n",
       "0     10189  19  461  \n",
       "0      2652   5   92  \n",
       "0      3169   6   97  \n",
       "0      2394   4  346  \n",
       "0      1482   2  458  \n",
       "\n",
       "[512 rows x 7 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grand_sectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557127"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grand_files_df.Q.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = sqlite3.connect(\"/Volumes/Samsung_T5/M57/pat.db\")\n",
    "grand_sectors_df.to_sql('block_hashes', con, index=True)\n",
    "grand_files_df.to_sql('files', con, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('scan_match_validate_all_mpi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9897bb9d3ea6b47cb59a3fa29a33cee83df786d8baa248772bb7eb2ce311867"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
