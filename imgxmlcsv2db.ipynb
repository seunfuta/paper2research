{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dfxml.fiwalk as fiwalk\n",
    "import dfxml \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import pprint\n",
    "from pprint import pprint\n",
    "imagefile = open(\"/Volumes/Samsung_T5/M57/pat-2009-12-10.raw.raw\")\n",
    "data = \"\"\n",
    "def process(fi): # fi=fileobject\n",
    "    #offset = fi.contents().find(data)\n",
    "    #if offset>0:\n",
    "    print(fi.filename())\n",
    "    \n",
    "objs = fiwalk.fileobjects_using_sax(imagefile=imagefile)\n",
    "print(len(objs))\n",
    "\n",
    "\n",
    "dfxml_path = \"/Volumes/Samsung_T5/M57/pat-2009-12-10.xml\"\n",
    "img_path = \"/Volumes/Samsung_T5/argo/pat2.csv\"\n",
    "\n",
    "id = 27964\n",
    "#Lets make sure all \"files\" are covered\n",
    "# Lets store it a DB\n",
    "\n",
    "\n",
    "\n",
    "files_cols = ['obj_id', 'partition','inode','filename','filesize']\n",
    "block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "file_df = pd.DataFrame(columns=files_cols)\n",
    "block_hashes_df = pd.DataFrame(columns=block_hashes_cols)\n",
    "\n",
    "sector_size = 512\n",
    "\n",
    "counter = 0\n",
    "print(len(objs))\n",
    "#with open(img_path, 'rb') as img_h:\n",
    "for obj in objs:\n",
    "    if int(obj._tags['id']) == id:# and obj.filesize() == False:\n",
    "        counter += 1\n",
    "        #print(obj.filename())\n",
    "        data = [obj._tags['id'], obj.partition(),obj.inode(), obj.filename(),  obj.filesize()]\n",
    "        file_df.loc[len(file_df.index)] = data\n",
    "        print(data)\n",
    "        byterun = []\n",
    "        persist_img_offset = 0\n",
    "        persist_fs_offset = 0\n",
    "        persist_file_offset = 0\n",
    "        print(obj.byte_runs())\n",
    "        for byterun1 in obj.byte_runs():\n",
    "            pprint(vars(byterun1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "        print(len(obj.byte_runs()))\n",
    "\n",
    "#for ebyterun in obj.byte_runs():\n",
    "#    pprint(vars(ebyterun))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 11580\n",
    "#Lets make sure all \"files\" are covered\n",
    "# Lets store it a DB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import pprint\n",
    "import math\n",
    "dfxml_path = \"/Volumes/Samsung_T5/M57/pat-2009-12-10.xml\"\n",
    "img_csv = \"/Volumes/Samsung_T5/argo/pat2.csv\"\n",
    "img_csv_df = pd.read_csv(img_csv)\n",
    "img_path =  \"/Volumes/Samsung_T5/M57/pat-2009-12-10.raw.raw\"\n",
    "files_cols = ['obj_id', 'partition','inode','filename','filesize']\n",
    "block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "file_df = pd.DataFrame(columns=files_cols)\n",
    "block_hashes_df = pd.DataFrame(columns=block_hashes_cols)\n",
    "\n",
    "sector_size = 512\n",
    "from pprint import pprint\n",
    "counter = 0\n",
    "print(len(objs))\n",
    "with open(img_path, 'rb') as img_h:\n",
    "    for obj in objs:\n",
    "        if int(obj._tags['id']) == id:# and obj.filesize() == False:\n",
    "            counter += 1\n",
    "            #print(obj.filename())\n",
    "            data = [obj._tags['id'], obj.partition(),obj.inode(), obj.filename(),  obj.filesize()]\n",
    "            file_df.loc[len(file_df.index)] = data\n",
    "            print(data)\n",
    "            byterun = []\n",
    "            persist_img_offset = 0\n",
    "            persist_fs_offset = 0\n",
    "            persist_file_offset = 0\n",
    "            remaining_len = obj.filesize()\n",
    "            ebyterun_df = pd.DataFrame(columns=['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1'])\n",
    "            for ebyterun in obj.byte_runs():\n",
    "                #pprint(vars(ebyterun)) \n",
    "                #byterun.append([ebyterun.img_offset,ebyterun.file_offset, ebyterun.len, ebyterun.fs_offset])\n",
    "                byterun.append(ebyterun.file_offset)\n",
    "                persist_file_offset = ebyterun.file_offset\n",
    "                byterun.append(ebyterun.len)\n",
    "                if hasattr(ebyterun,'img_offset'): \n",
    "                    byterun.append(ebyterun.img_offset)\n",
    "                    if ebyterun.img_offset != None: persist_img_offset = ebyterun.img_offset\n",
    "                if hasattr(ebyterun,'fs_offset'): \n",
    "                    byterun.append(ebyterun.fs_offset)\n",
    "                    if ebyterun.fs_offset != None: persist_fs_offset = ebyterun.fs_offset\n",
    "                if hasattr(ebyterun,'len'): \n",
    "                    byterun.append(ebyterun.len)\n",
    "                    if ebyterun.len != None: persist_len = ebyterun.len\n",
    "                #print(\"img_offset\", img_offset, \"file offset\", ebyterun.file_offset)\n",
    "                #print(\"file_offset\", \"len\", \"img_offset\", \"fs_offset\")\n",
    "                byterun_start = int(persist_img_offset / sector_size)\n",
    "                byterun_end = int((math.ceil((persist_img_offset + ebyterun.len) / sector_size))) - 1\n",
    "                len_run = np.arange(persist_len,0,-sector_size) #remaining_len\n",
    "                sector_run = np.full(len(len_run), sector_size)\n",
    "                len_run = np.minimum(len_run,sector_run)\n",
    "                #len_run = min(sector_size,len_run.all())\n",
    "                print(\"len_run\",len(len_run), len(len_run)*sector_size)\n",
    "                print(\"byterun_start\", byterun_start,persist_img_offset, \"byterun_end\", byterun_end,  \"len\", ebyterun.len)\n",
    "                print(\"ebyterun_df\", ebyterun_df.shape, \"img_csv\", img_csv_df.loc[byterun_start:byterun_end,'img_offset'].shape )\n",
    "                #print(img_csv_df.loc[byterun_start:byterun_end,:])\n",
    "                print(\"img_csv_len\",len(img_csv_df.loc[byterun_start:byterun_end, :]))\n",
    "                ebyterun_df.loc[:,'img_sector_offset'] = np.arange(len(img_csv_df))\n",
    "                ebyterun_df.loc[byterun_start:byterun_end,'obj_id'] = obj._tags['id']\n",
    "                ebyterun_df.loc[byterun_start:byterun_end,'img_offset'] = img_csv_df.loc[byterun_start:byterun_end,'img_offset']\n",
    "                ebyterun_df.loc[byterun_start:byterun_end,'fs_offset'] = np.arange(persist_fs_offset,persist_fs_offset+ebyterun.len,sector_size)\n",
    "                ebyterun_df.loc[byterun_start:byterun_end,'file_offset'] = np.arange(persist_file_offset,persist_file_offset+ebyterun.len, sector_size)\n",
    "                ebyterun_df.loc[byterun_start:byterun_end,'len'] = len_run\n",
    "                #print(\"remaining_len\",remaining_len)\n",
    "                #remaining_len-=sector_size\n",
    "                ebyterun_df.loc[byterun_start:byterun_end,'md5'] = img_csv_df.loc[byterun_start:byterun_end,'md5']\n",
    "            print(ebyterun_df)    \n",
    "'''\n",
    "                remaining_len = ebyterun.len\n",
    "                #cur_file_offset = 0\n",
    "                #print(\"image offset\", persist_img_offset)\n",
    "                #print(\"file offset\", ebyterun.file_offset)\n",
    "                #print(\"len\", ebyterun.len)\n",
    "                #print()\n",
    "                for pointer in range(persist_img_offset,persist_img_offset+ebyterun.len, sector_size):\n",
    "                    #print(pointer)\n",
    "                    persist_img_offset = pointer\n",
    "                    \n",
    "                    img_h.seek(pointer)\n",
    "                    fsector = img_h.read(sector_size)\n",
    "                    \n",
    "                    hash_len = min(remaining_len,sector_size)\n",
    "                    \n",
    "                    sector_md5 = hashlib.md5(fsector).hexdigest()\n",
    "                    sector_sha1 = hashlib.sha1(fsector).hexdigest()\n",
    "                    data2 = [obj._tags['id'], pointer, persist_fs_offset, persist_file_offset, hash_len, sector_md5, sector_sha1]\n",
    "                    persist_fs_offset+=sector_size \n",
    "                    persist_file_offset+=sector_size \n",
    "                    remaining_len -= sector_size\n",
    "                    block_hashes_df.loc[len(block_hashes_df.index)] = data2\n",
    "\n",
    "                print(counter, \" out of \", len(objs), \"id= \",obj._tags['id'])\n",
    "\n",
    "    #print(len(file_df))\n",
    "conn = sqlite3.connect(\"/Users/seunfuta/Downloads/M57/charlie-2009-12-11.db\")\n",
    "cur = conn.cursor()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS files (obj_id INTEGER, partition INTEGER, inode INTEGER, filename TEXT, filesize INTEGER)')\n",
    "conn.commit()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS block_hashes (obj_id INTEGER, img_offset INTEGER,fs_offset INTEGER, file_offset INTEGER,len INTEGER, md5 TEXT,sha1 TEXT)')\n",
    "conn.commit()\n",
    "file_df.to_sql('files', conn, if_exists='replace', index = False)\n",
    "block_hashes_df.to_sql('block_hashes', conn, if_exists='replace', index = False)\n",
    "conn.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_id</th>\n",
       "      <th>img_offset</th>\n",
       "      <th>fs_offset</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>len</th>\n",
       "      <th>md5</th>\n",
       "      <th>sha1</th>\n",
       "      <th>img_sector_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13558039</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941715968</td>\n",
       "      <td>6941683712</td>\n",
       "      <td>0</td>\n",
       "      <td>512</td>\n",
       "      <td>39dcab5c1d131f2a84db06959ae4a4d7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558040</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941716480</td>\n",
       "      <td>6941684224</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>111ef107791fd7ab654a04e91135a072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558041</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941716992</td>\n",
       "      <td>6941684736</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>dbf0e745da11cd5810f2189cc2587534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558042</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941717504</td>\n",
       "      <td>6941685248</td>\n",
       "      <td>1536</td>\n",
       "      <td>512</td>\n",
       "      <td>274c3ccd287a94479a60414a1686cd42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558043</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941718016</td>\n",
       "      <td>6941685760</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>227b5a49b52935d27e6f2ed538161d6e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558044</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941718528</td>\n",
       "      <td>6941686272</td>\n",
       "      <td>2560</td>\n",
       "      <td>512</td>\n",
       "      <td>7d67abbcd8d5c7b03d6006bf00f7562a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558045</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941719040</td>\n",
       "      <td>6941686784</td>\n",
       "      <td>3072</td>\n",
       "      <td>512</td>\n",
       "      <td>90af5b3c60b4ccd524ade98ecb5f0691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558046</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941719552</td>\n",
       "      <td>6941687296</td>\n",
       "      <td>3584</td>\n",
       "      <td>121</td>\n",
       "      <td>fb6f3f4d96419ea9b887ca9defa28911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         obj_id  img_offset   fs_offset file_offset  len  \\\n",
       "13558039   1211  6941715968  6941683712           0  512   \n",
       "13558040   1211  6941716480  6941684224         512  512   \n",
       "13558041   1211  6941716992  6941684736        1024  512   \n",
       "13558042   1211  6941717504  6941685248        1536  512   \n",
       "13558043   1211  6941718016  6941685760        2048  512   \n",
       "13558044   1211  6941718528  6941686272        2560  512   \n",
       "13558045   1211  6941719040  6941686784        3072  512   \n",
       "13558046   1211  6941719552  6941687296        3584  121   \n",
       "\n",
       "                                       md5 sha1  img_sector_offset  \n",
       "13558039  39dcab5c1d131f2a84db06959ae4a4d7  NaN           13558039  \n",
       "13558040  111ef107791fd7ab654a04e91135a072  NaN           13558040  \n",
       "13558041  dbf0e745da11cd5810f2189cc2587534  NaN           13558041  \n",
       "13558042  274c3ccd287a94479a60414a1686cd42  NaN           13558042  \n",
       "13558043  227b5a49b52935d27e6f2ed538161d6e  NaN           13558043  \n",
       "13558044  7d67abbcd8d5c7b03d6006bf00f7562a  NaN           13558044  \n",
       "13558045  90af5b3c60b4ccd524ade98ecb5f0691  NaN           13558045  \n",
       "13558046  fb6f3f4d96419ea9b887ca9defa28911  NaN           13558046  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebyterun_df.loc[13558039:13558046,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_id</th>\n",
       "      <th>img_offset</th>\n",
       "      <th>fs_offset</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>len</th>\n",
       "      <th>md5</th>\n",
       "      <th>sha1</th>\n",
       "      <th>img_sector_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13558039</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941715968</td>\n",
       "      <td>6941683712</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>39dcab5c1d131f2a84db06959ae4a4d7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558040</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941716480</td>\n",
       "      <td>6941684224</td>\n",
       "      <td>512</td>\n",
       "      <td>True</td>\n",
       "      <td>111ef107791fd7ab654a04e91135a072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558041</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941716992</td>\n",
       "      <td>6941684736</td>\n",
       "      <td>1024</td>\n",
       "      <td>True</td>\n",
       "      <td>dbf0e745da11cd5810f2189cc2587534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558042</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941717504</td>\n",
       "      <td>6941685248</td>\n",
       "      <td>1536</td>\n",
       "      <td>True</td>\n",
       "      <td>274c3ccd287a94479a60414a1686cd42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558043</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941718016</td>\n",
       "      <td>6941685760</td>\n",
       "      <td>2048</td>\n",
       "      <td>True</td>\n",
       "      <td>227b5a49b52935d27e6f2ed538161d6e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558044</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941718528</td>\n",
       "      <td>6941686272</td>\n",
       "      <td>2560</td>\n",
       "      <td>True</td>\n",
       "      <td>7d67abbcd8d5c7b03d6006bf00f7562a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558045</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941719040</td>\n",
       "      <td>6941686784</td>\n",
       "      <td>3072</td>\n",
       "      <td>True</td>\n",
       "      <td>90af5b3c60b4ccd524ade98ecb5f0691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558046</th>\n",
       "      <td>1211</td>\n",
       "      <td>6941719552</td>\n",
       "      <td>6941687296</td>\n",
       "      <td>3584</td>\n",
       "      <td>True</td>\n",
       "      <td>fb6f3f4d96419ea9b887ca9defa28911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13558046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         obj_id  img_offset   fs_offset file_offset   len  \\\n",
       "13558039   1211  6941715968  6941683712           0  True   \n",
       "13558040   1211  6941716480  6941684224         512  True   \n",
       "13558041   1211  6941716992  6941684736        1024  True   \n",
       "13558042   1211  6941717504  6941685248        1536  True   \n",
       "13558043   1211  6941718016  6941685760        2048  True   \n",
       "13558044   1211  6941718528  6941686272        2560  True   \n",
       "13558045   1211  6941719040  6941686784        3072  True   \n",
       "13558046   1211  6941719552  6941687296        3584  True   \n",
       "\n",
       "                                       md5 sha1  img_sector_offset  \n",
       "13558039  39dcab5c1d131f2a84db06959ae4a4d7  NaN           13558039  \n",
       "13558040  111ef107791fd7ab654a04e91135a072  NaN           13558040  \n",
       "13558041  dbf0e745da11cd5810f2189cc2587534  NaN           13558041  \n",
       "13558042  274c3ccd287a94479a60414a1686cd42  NaN           13558042  \n",
       "13558043  227b5a49b52935d27e6f2ed538161d6e  NaN           13558043  \n",
       "13558044  7d67abbcd8d5c7b03d6006bf00f7562a  NaN           13558044  \n",
       "13558045  90af5b3c60b4ccd524ade98ecb5f0691  NaN           13558045  \n",
       "13558046  fb6f3f4d96419ea9b887ca9defa28911  NaN           13558046  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebyterun_df.loc[13558039:13558046,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        '''\n",
    "        for ebyterun in obj.byte_runs():\n",
    "            pprint(vars(ebyterun)) \n",
    "        \n",
    "            byterun.append(ebyterun.file_offset)\n",
    "            persist_file_offset = ebyterun.file_offset\n",
    "            byterun.append(ebyterun.len)\n",
    "            if hasattr(ebyterun,'img_offset'): \n",
    "                byterun.append(ebyterun.img_offset)\n",
    "                if ebyterun.img_offset != None: persist_img_offset = ebyterun.img_offset\n",
    "            if hasattr(ebyterun,'fs_offset'): \n",
    "                byterun.append(ebyterun.fs_offset)\n",
    "                if ebyterun.fs_offset != None: persist_fs_offset = ebyterun.fs_offset\n",
    "\n",
    "            \n",
    "            remaining_len = ebyterun.len\n",
    "\n",
    "            for pointer in range(persist_img_offset,persist_img_offset+ebyterun.len, sector_size):\n",
    "                #print(pointer)\n",
    "                persist_img_offset = pointer\n",
    "                \n",
    "                img_h.seek(pointer)\n",
    "                fsector = img_h.read(sector_size)\n",
    "                \n",
    "                hash_len = min(remaining_len,sector_size)\n",
    "                \n",
    "                sector_md5 = hashlib.md5(fsector).hexdigest()\n",
    "                sector_sha1 = hashlib.sha1(fsector).hexdigest()\n",
    "                data2 = [obj._tags['id'], pointer, persist_fs_offset, persist_file_offset, hash_len, sector_md5, sector_sha1]\n",
    "                persist_fs_offset+=sector_size \n",
    "                persist_file_offset+=sector_size \n",
    "                remaining_len -= sector_size\n",
    "                block_hashes_df.loc[len(block_hashes_df.index)] = data2\n",
    "            \n",
    "            print(counter, \" out of \", len(objs), \"id= \",obj._tags['id'])\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        '''\n",
    "#print(len(file_df))\n",
    "'''\n",
    "conn = sqlite3.connect(\"/Volumes/Samsung_T5/M57/pat-2009-12-10.db\")\n",
    "cur = conn.cursor()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS files (obj_id INTEGER, partition INTEGER, inode INTEGER, filename TEXT, filesize INTEGER)')\n",
    "conn.commit()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS block_hashes (obj_id INTEGER, img_offset INTEGER,fs_offset INTEGER, file_offset INTEGER,len INTEGER, md5 TEXT,sha1 TEXT)')\n",
    "conn.commit()\n",
    "file_df.to_sql('files', conn, if_exists='replace', index = False)\n",
    "block_hashes_df.to_sql('block_hashes', conn, if_exists='replace', index = False)\n",
    "conn.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fi(self,fi):\n",
    "    global options\n",
    "    # Filter out specific filenames create by TSK that are not of use\n",
    "    if ignore_filename(fi.filename(), self.include_dotdirs):\n",
    "        return \n",
    "\n",
    "    dprint(\"processing %s\" % str(fi))\n",
    "    \n",
    "    # See if the filename changed its hash code\n",
    "    changed = False\n",
    "    if not fi.allocated():\n",
    "        return # only look at allocated files\n",
    "\n",
    "    # Remember the file for the next generation\n",
    "    self.new_fnames[fi.filename()] = fi\n",
    "    self.new_inodes[(fi.partition(), fi.inode())] = fi\n",
    "    self.new_fi_tally += 1\n",
    "\n",
    "    # See if a file with this filename had its contents change or properties changed\n",
    "    ofi = self.fnames.get(fi.filename(),None)\n",
    "    if ofi:\n",
    "        dprint(\"   found ofi\")\n",
    "        any_diff = False\n",
    "        if ofi.sha1()!=fi.sha1():\n",
    "            dprint(\"      >>> sha1 changed\")\n",
    "            self.changed_content.add((ofi,fi))\n",
    "            any_diff = True\n",
    "        elif ofi.atime() != fi.atime() or \\\n",
    "                ofi.mtime() != fi.mtime() or \\\n",
    "                ofi.crtime() != fi.crtime() or \\\n",
    "                ofi.ctime() != fi.ctime():\n",
    "            dprint(\"      >>> time changed\")\n",
    "            self.changed_properties.add((ofi,fi))\n",
    "            any_diff = True\n",
    "\n",
    "        if any_diff:\n",
    "            #Count the types of changes that happened\n",
    "            if ofi.filesize() != fi.filesize():\n",
    "                self.changed_filesize_tally += 1\n",
    "            if ofi.sha1() != fi.sha1():\n",
    "                if ofi.is_dir():\n",
    "                    self.changed_dir_sha1_tally += 1\n",
    "                elif ofi.is_file():\n",
    "                    self.changed_file_sha1_tally += 1\n",
    "            if ofi.mtime() != fi.mtime():\n",
    "                self.changed_mtime_tally += 1\n",
    "            if ofi.atime() != fi.atime():\n",
    "                self.changed_atime_tally += 1\n",
    "            if ofi.ctime() != fi.ctime():\n",
    "                self.changed_ctime_tally += 1\n",
    "            if ofi.crtime() != fi.crtime():\n",
    "                self.changed_crtime_tally += 1\n",
    "            if ofi.byte_runs() and fi.byte_runs():\n",
    "                brdiff = 0\n",
    "                ofirstbr = ofi.byte_runs()[0]\n",
    "                nfirstbr =  fi.byte_runs()[0]\n",
    "                try:\n",
    "                    if ofirstbr.file_offset == nfirstbr.file_offset:\n",
    "                        brdiff = 1\n",
    "                    if ofirstbr.img_offset == nfirstbr.img_offset:\n",
    "                        brdiff = 1\n",
    "                    if ofirstbr.fs_offset == nfirstbr.fs_offset:\n",
    "                        brdiff = 1\n",
    "                except:\n",
    "                    pass\n",
    "                self.changed_first_byterun_tally += brdiff\n",
    "        \n",
    "\n",
    "    # If a new file, note that (and optionally add to the timeline)\n",
    "    if not ofi:\n",
    "        self.new_files.add(fi)\n",
    "        if self.timeline:\n",
    "            create_time = fi.crtime()\n",
    "            if not create_time: create_time = fi.ctime()\n",
    "            if not create_time: create_time = fi.mtime()\n",
    "            self.timeline.add((create_time,fi.filename(),\"created\"))\n",
    "\n",
    "    # Delete files we have seen (so we can find out the files that were deleted)\n",
    "    if fi.filename() in self.fnames:\n",
    "        del self.fnames[fi.filename()]\n",
    "\n",
    "    # Look for files that were renamed\n",
    "    ofi = self.inodes.get((fi.partition(), fi.inode()),None)\n",
    "    if ofi and ofi.filename() != fi.filename() and ofi.sha1()==fi.sha1():\n",
    "        #Never consider current-directory or parent-directory for rename operations.  Because we match on partition+inode numbers, these trivially match.\n",
    "        if not (fi.filename().endswith(\"/.\") or fi.filename().endswith(\"/..\") or ofi.filename().endswith(\"/.\") or ofi.filename().endswith(\"/..\")):\n",
    "            self.renamed_files.add((ofi,fi))\n",
    "\n",
    "def process(self,fname):\n",
    "    self.prior_fname = self.current_fname\n",
    "    self.current_fname = fname\n",
    "    if fname.endswith(\"xml\"):\n",
    "        with open(fname,'rb') as xmlfile:\n",
    "            for fi in dfxml.iter_dfxml(xmlfile, preserve_elements=True):\n",
    "                self.process_fi(fi)\n",
    "    else:\n",
    "        fiwalk.fiwalk_using_sax(imagefile=open(fname,'rb'), flags=fiwalk.ALLOC_ONLY, callback=self.process_fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Lets make sure all \"files\" are covered\n",
    "# Lets store it a DB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import pprint\n",
    "import math\n",
    "from pprint import pprint\n",
    "import dfxml \n",
    "\n",
    "\n",
    "id = 11580\n",
    "dfxml_path = \"/Volumes/Samsung_T5/M57/pat-2009-12-10.raw.xml\"\n",
    "img_csv = \"/Volumes/Samsung_T5/argo/pat2.csv\"\n",
    "img_csv_df = pd.read_csv(img_csv)\n",
    "img_path =  \"/Volumes/Samsung_T5/M57/pat-2009-12-10.raw.raw\"\n",
    "files_cols = ['obj_id', 'partition','inode','filename','filesize']\n",
    "block_hashes_cols = ['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1']\n",
    "file_df = pd.DataFrame(columns=files_cols)\n",
    "block_hashes_df = pd.DataFrame(columns=block_hashes_cols)\n",
    "\n",
    "sector_size = 512\n",
    "counter = 0\n",
    "#print(len(objs))\n",
    "\n",
    "def process_fi(obj):\n",
    "    global options\n",
    "    # Filter out specific filenames create by TSK that are not of use\n",
    "    if not obj.allocated():\n",
    "        return\n",
    "    print(\"processing %s\" % str(obj))\n",
    "    \n",
    "    # See if the filename changed its hash code\n",
    "    changed = False\n",
    "    if not obj.allocated():\n",
    "        return # only look at allocated files\n",
    "\n",
    "    if int(obj._tags['id']) == id:# and obj.filesize() == False:\n",
    "        #counter += 1\n",
    "        #print(obj.filename())\n",
    "        data = [obj._tags['id'], obj.partition(),obj.inode(), obj.filename(),  obj.filesize()]\n",
    "        file_df.loc[len(file_df.index)] = data\n",
    "        print(data)\n",
    "        byterun = []\n",
    "        persist_img_offset = 0\n",
    "        persist_fs_offset = 0\n",
    "        persist_file_offset = 0\n",
    "        remaining_len = obj.filesize()\n",
    "        ebyterun_df = pd.DataFrame(columns=['obj_id','img_offset','fs_offset','file_offset','len','md5','sha1'])\n",
    "        for ebyterun in obj.byte_runs():\n",
    "            #pprint(vars(ebyterun)) \n",
    "            #byterun.append([ebyterun.img_offset,ebyterun.file_offset, ebyterun.len, ebyterun.fs_offset])\n",
    "            byterun.append(ebyterun.file_offset)\n",
    "            persist_file_offset = ebyterun.file_offset\n",
    "            byterun.append(ebyterun.len)\n",
    "            if hasattr(ebyterun,'img_offset'): \n",
    "                byterun.append(ebyterun.img_offset)\n",
    "                if ebyterun.img_offset != None: persist_img_offset = ebyterun.img_offset\n",
    "            if hasattr(ebyterun,'fs_offset'): \n",
    "                byterun.append(ebyterun.fs_offset)\n",
    "                if ebyterun.fs_offset != None: persist_fs_offset = ebyterun.fs_offset\n",
    "            if hasattr(ebyterun,'len'): \n",
    "                byterun.append(ebyterun.len)\n",
    "                if ebyterun.len != None: persist_len = ebyterun.len\n",
    "            #print(\"img_offset\", img_offset, \"file offset\", ebyterun.file_offset)\n",
    "            #print(\"file_offset\", \"len\", \"img_offset\", \"fs_offset\")\n",
    "            byterun_start = int(persist_img_offset / sector_size)\n",
    "            byterun_end = int((math.ceil((persist_img_offset + ebyterun.len) / sector_size))) - 1\n",
    "            len_run = np.arange(persist_len,0,-sector_size) #remaining_len\n",
    "            sector_run = np.full(len(len_run), sector_size)\n",
    "            len_run = np.minimum(len_run,sector_run)\n",
    "            #len_run = min(sector_size,len_run.all())\n",
    "            print(\"len_run\",len(len_run), len(len_run)*sector_size)\n",
    "            print(\"byterun_start\", byterun_start,persist_img_offset, \"byterun_end\", byterun_end,  \"len\", ebyterun.len)\n",
    "            print(\"ebyterun_df\", ebyterun_df.shape, \"img_csv\", img_csv_df.loc[byterun_start:byterun_end,'img_offset'].shape )\n",
    "            #print(img_csv_df.loc[byterun_start:byterun_end,:])\n",
    "            print(\"img_csv_len\",len(img_csv_df.loc[byterun_start:byterun_end, :]))\n",
    "            ebyterun_df.loc[:,'img_sector_offset'] = np.arange(len(img_csv_df))\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'obj_id'] = obj._tags['id']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'img_offset'] = img_csv_df.loc[byterun_start:byterun_end,'img_offset']\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'fs_offset'] = np.arange(persist_fs_offset,persist_fs_offset+ebyterun.len,sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'file_offset'] = np.arange(persist_file_offset,persist_file_offset+ebyterun.len, sector_size)\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'len'] = len_run\n",
    "            #print(\"remaining_len\",remaining_len)\n",
    "            #remaining_len-=sector_size\n",
    "            ebyterun_df.loc[byterun_start:byterun_end,'md5'] = img_csv_df.loc[byterun_start:byterun_end,'md5']\n",
    "        print(ebyterun_df)    \n",
    "\n",
    "\n",
    "def process(fname):\n",
    "    if fname.endswith(\"xml\"):\n",
    "        with open(fname,'rb') as xmlfile:\n",
    "            for fi in dfxml.iter_dfxml(xmlfile, preserve_elements=True):\n",
    "                process_fi(fi)\n",
    "    else:\n",
    "        fiwalk.fiwalk_using_sax(imagefile=open(fname,'rb'), flags=fiwalk.ALLOC_ONLY, callback=self.process_fi)\n",
    "#objs = fiwalk.fileobjects_using_sax(imagefile=imagefile)\n",
    "\n",
    "#with open(img_path, 'rb') as img_h:\n",
    "process(dfxml_path)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('paper2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1895fe77d861caeaa821f2df72086efbb9754c9ce180e62170f24c0ec4bbaef9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
